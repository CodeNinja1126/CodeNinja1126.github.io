I"<h1 id="18일차---nlp-sequence-to-sequence-with-attention">18일차 - NLP Sequence to Sequence with Attention</h1>

<h1 id="seq2seq-model">Seq2Seq Model</h1>

<ul>
  <li>단어의 시퀀스를 인풋으로 받고 다시 시퀀스를 아웃풋으로 준다.</li>
  <li>인코더와 디코더로 구성되어있다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled.png" alt="Untitled.png" /></p>
<ul>
  <li>이 때 특정 디코더에서는 시작과 끝에 특정 토큰을 입력과 출력으로 사용하는데 이를 <SOS> (문장의 시작으로 입력), <EOS> (문장의 끝으로 출력)라고 부른다.</EOS></SOS></li>
  <li>어텐션 모델의 기본적인 아이디어는 디코더로 $h_t$벡터 하나만 넘겨주는 것이 아니라, 인코더의 각 단어의 정보가 짙게 담겨있는 $h_1^{(e)}, h_2^{(e)},…,h_n^{(e)}$  벡터를 넘겨주고 이를 디코더에서 선별적으로 사용하는 것이다.</li>
</ul>

<h1 id="attention">Attention</h1>

<ul>
  <li>어텐션은 각 단어의 뜻이 희석되는 문제를 해결한다.</li>
  <li>어텐션 분포는 인코더의 $h$벡터의 가중치가 된다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled%201.png" alt="Untitled%201.png" /></p>
<ul>
  <li>계산의 세부 과정은 이렇다. 디코더의 $h_d^{(d)}$벡터와 인코더의 각 $h_1^{(e)}, h_2^{(e)},…,h_n^{(e)}$ 들을 내적해 attention score를 계산한다.</li>
  <li>그리고 그 값들을 소프트맥스 함수를 이용해 attention distribution을 계산하고 그 값을 가중치로 $h_1^{(e)}, h_2^{(e)},…,h_n^{(e)}$ 를 각각 곱해 아웃풋을 낸다.</li>
  <li>이 때 attention distribution을 attention vector라고 한다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled%202.png" alt="Untitled%202.png" /></p>
<ul>
  <li>만들어진 attention output과 디코더의 $h$벡터를 concat해  $\hat y_1$을 만드는데 사용한다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled%203.png" alt="Untitled%203.png" /></p>
<ul>
  <li>초록색은 순전파의 방향, 보라색은 역전파의 방향이다.</li>
  <li>이 모델을 학습시킬 때 정석적인 방법은 디코더의 예측값을 그 다음 시퀀스의 입력값으로 사용해 학습시키는 것이지만 이는 매우 학습이 더디다. 그래서 학습 초반에는 정답 레이블을 디코더의 입력으로 사용해 학습시키는데 이를 Teacher forcing라고 한다.</li>
  <li>다만 이 Teacher forcing은 실제 모델이 사용되는 환경과 다르므로 제대로 학습이 안 될 수 있다. 그래서 처음에는 Teacher forcing를 사용하고 어느 정도 정답율이 오르면 그 때 정석적인 방법으로 예측값을 디코더에 입력해 모델을 학습시키게 된다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled%204.png" alt="Untitled%204.png" /></p>
<ul>
  <li>general은 $W_a$벡터를 사용해 각 벡터의 어떤 값들의 가중치를 둘 것인지를 파라미터를 줄 수 있다.</li>
  <li>concat는 $h_t$와 $\bar h_s$를 concat 시킨다음 가중치를 곱하고 tanh의 비선형 모델을 통과시켜 마지막으로 $v_a$와 곱해 스칼라 곱으로 만든다.</li>
</ul>

<h1 id="beam-search">Beam search</h1>

<p><img src="/images/2021-02-18/032/Untitled%205.png" alt="Untitled%205.png" /></p>
<ul>
  <li>그 때 그 때 가장 그럴듯한 단어를 선택하는 그리디 디코딩은 결정을 번복할 수 없다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled%206.png" alt="Untitled%206.png" /></p>
<ul>
  <li>Exhausitive search는 모든 단어를 추적해야 하므로 너무 많은 연산 자원을 필요로 한다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled%207.png" alt="Untitled%207.png" /></p>
<ul>
  <li>그래서 가장 점수가 높은 k개의 단어를 구해 각 스텝마다 k개의 단어를 추적한다.</li>
  <li>이 방법은 전체적으로 최적화된 결과를 보장하지는 않지만 exhausitive search보다는 훨씬 효율적이다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled%208.png" alt="Untitled%208.png" /></p>
<ul>
  <li>이와 같이 가지를 치되 항상 가장 높은 확률 값을 가지는 k개의 단어만을 추적한다.</li>
  <li>이 때, 추적 가설마다 <END> 토큰을 출력하는 길이가 다를 수 있다. <END>토큰을 출력하면 그 가설을 저장하고 다시 가설의 수를 k개로 유지하며, 추적을 계속한다.</END></END></li>
  <li>추적이 끝나는 때는 미리 설정된 최대 길이인 T에 도달하거나, <END> 토큰을 반환한 가설이 n개(미리 설정된)가 됐을 때이다.</END></li>
  <li>하지만 이 때  문장 길이가 길면 필연적으로 확률 값이 작아질 수 밖에 없다. 그래서 모든 더한 확률값을 가설의 길이 t로 나누어 준다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled%209.png" alt="Untitled%209.png" /></p>
<h1 id="bleu-score">BLEU score</h1>

<p><img src="/images/2021-02-18/032/Untitled%2010.png" alt="Untitled%2010.png" /></p>
<ul>
  <li>각 위치에 상관 없이 존재하는 단어만으로 정확도를 잰다.</li>
  <li><strong>precision</strong> : 예측된 정보가 얼마나 정확한가에 대한 값</li>
  <li><strong>recall :</strong> 예측된 정보가 얼마나 원래의 값을 재현하는가</li>
  <li>F - measure는 조화평균을 이야기한다.</li>
  <li>하지만 위의 점수는 순서를 보장하지 않으므로 단어만 일치하고 순서는 엉망인 이상한 문장이 높은 점수를 받을 수 있다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled%2011.png" alt="Untitled%2011.png" /></p>
<ul>
  <li>그래서 BLEU는 N-gram을 설정해 gram 단위로 precision을 계산한다.</li>
  <li>이 때는 기하평균을 이용해 평균값을 계산한다.</li>
  <li>brevity penalty는 recall값을 평가에 반영하기 위한 값으로, 만약 예측된 값의 재현도가 높다면 길이가 레퍼런스와 비슷할 것이다. 따라서 길이가 비슷할 경우 높은 가중치를 주도록 하는 것이다.</li>
  <li>각 n-gram 및 BLEU를 계산한 값은 다음과 같다.</li>
</ul>

<p><img src="/images/2021-02-18/032/Untitled%2012.png" alt="Untitled%2012.png" /></p>
:ET