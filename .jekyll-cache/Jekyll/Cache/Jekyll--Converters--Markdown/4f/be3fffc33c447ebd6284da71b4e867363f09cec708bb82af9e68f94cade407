I"“&<h1 id="optimization">Optimization</h1>

<h2 id="gradient-descent">Gradient Descent</h2>

<ul>
  <li>First-order iterative optimization algorithm for finding a local minimum of a differentiable function.</li>
</ul>

<h2 id="important-concepts-in-optimization">Important Concepts in Optimization</h2>

<h3 id="generalization">Generalization</h3>

<ul>
  <li>How well the learned model will behave on unseen data.</li>
  <li>í•™ìŠµ ë°ì´í„° ì‚¬ì´ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ì´ì˜ ì°¨ì´</li>
</ul>

<h3 id="underfitting-vs-overfitting">Underfitting vs. Overfitting</h3>

<p><img src="/images/2021-02-03/26/Untitled.png" alt="Untitled.png" /></p>
<h3 id="cross-validation">Cross-validation</h3>

<ul>
  <li>Cross-validation is a model validation technique for assessing how the model will generalize to an independent (test) data set.</li>
  <li>ë¡œìŠ¤ í•¨ìˆ˜ì˜ ì„ íƒì´ë‚˜ ë ˆì´ì–´ì˜ ê°¯ìˆ˜ ê°™ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ë¹„êµí•  ë•Œ ì‚¬ìš©í•œë‹¤.</li>
  <li>í…ŒìŠ¤íŠ¸ ë°ì´íƒ€ëŠ” ì–´ë–¤ ì‹ìœ¼ë¡œë“  í•™ìŠµì— ì‚¬ìš©í•´ì„œëŠ” ì•ˆëœë‹¤.</li>
</ul>

<h3 id="bias-and-variance">Bias and Variance</h3>

<ul>
  <li>ë¹„ìŠ·í•œ ì…ë ¥ì— ë¹„ìŠ·í•œ ì¶œë ¥ì„ ê°€ì§€ëŠ” ê²ƒì„ Varianceê°€ ë‚®ë‹¤ê³  í•œë‹¤.</li>
  <li>ì¶œë ¥ì˜ meanì´ íƒ€ì¼“ë³´ë‹¤ ë§ì´ í´ê²½ìš° biasê°€ í¬ë‹¤ê³  í•œë‹¤.</li>
  <li>We can derive that what we are minimizing(<strong>cost</strong>) can be decomposed into the three different parts: bias$^2$, variance, and noise.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%201.png" alt="Untitled%201.png" /></p>
<ul>
  <li>ê²°ê³¼ì ìœ¼ë¡œ biasì™€ varianceë¥¼ ëª¨ë‘ ë‚®ì¶œ ìˆ˜ëŠ” ì—†ìœ¼ë©° íƒ€í˜‘ì„ ë´ì•¼ í•œë‹¤.</li>
</ul>

<h3 id="bootstrapping">Bootstrapping</h3>

<ul>
  <li>Bootstrapping is any test or metric that uses random sampling with replacement.</li>
  <li>ì „ì²´ ë°ì´í„°ì—ì„œ ì„ì˜ì˜ ìƒ˜í”Œì„ í•™ìŠµì‹œí‚¨ ì—¬ëŸ¬ ëª¨ë¸ì„ ê°€ì§€ê³  ì¼ê´€ì„±ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ê²ƒ</li>
</ul>

<h3 id="bagging-vs-boosting">Bagging vs. Boosting</h3>

<ul>
  <li><strong>Bagging</strong>(<strong>B</strong>oostrapping <strong>agg</strong>regat<strong>ing</strong>)</li>
  <li>Multiple models are being trained with bootstrapping.</li>
  <li>ex) Base classifiers are fitted on random subset where individual predictions are aggregated (votiong or averaging).</li>
  <li>Boosting</li>
  <li>It focuses on those specific training samples that are hard to classify.</li>
  <li>a strong model is built by combining weak learners in sequence where each learner learns from the mistakes of the previous weak learner.</li>
  <li>Boostingì€ ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ í•˜ë‚˜ì˜ ê°•í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ì‹ì´ë‹¤.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%202.png" alt="Untitled%202.png" /></p>
<h1 id="gradient-descent-methods">Gradient Descent Methods</h1>

<ul>
  <li>Stochastic Gradient Descent : update with the gradient computed from a single sample.</li>
  <li>Mini-batch Gradient Descent : update with the gradient computed from a subset of data.</li>
  <li>Batch Gradient Descent : update with the gradient computed from the whole data.</li>
</ul>

<h3 id="batch-size-matters">Batch-size Matters</h3>

<p><img src="/images/2021-02-03/26/Untitled%203.png" alt="Untitled%203.png" /></p>
<ul>
  <li>batch ì‚¬ì´ì¦ˆê°€ ì»¤ì§€ë©´ ì¼ë°˜ì ìœ¼ë¡œ sharp minimumìœ¼ë¡œ ëª¨ì´ëŠ” ê²½í–¥ì´ ìˆë‹¤.</li>
  <li>ë°˜ë©´ì— batch ì‚¬ì´ì¦ˆê°€ ì‘ìœ¼ë©´ ì¼ê´€ì ìœ¼ë¡œ flat minimumìœ¼ë¡œ ëª¨ì´ê²Œ ëœë‹¤.</li>
  <li>ê·¸ë˜í”„ì—ì„œ ë³´ë‹¤ì‹œí”¼ ì˜¤ì°¨ê°€ ìƒê¸¸ ë•Œ flat minimumì´ ì˜¤ì°¨ê°€ ë” ì ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
</ul>

<h2 id="gradient-descent-methods-1">Gradient Descent Methods</h2>

<h3 id="gradient-descent-1">Gradient Descent</h3>

<p><img src="/images/2021-02-03/26/Untitled%204.png" alt="Untitled%204.png" /></p>
<h3 id="momentum">Momentum</h3>

<p><img src="/images/2021-02-03/26/Untitled%205.png" alt="Untitled%205.png" /></p>
<h3 id="nesterov-accelerated-gradient">Nesterov Accelerated Gradient</h3>

<p><img src="/images/2021-02-03/26/Untitled%206.png" alt="Untitled%206.png" /></p>
<h3 id="adagrad">Adagrad</h3>

<ul>
  <li><strong>Adagrad</strong> adapts the learning rate, performing larger updates for infrequent and smaller updates for frequent parameters.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%207.png" alt="Untitled%207.png" /></p>
<ul>
  <li>í•™ìŠµì´ ë˜ë©´ ë  ìˆ˜ë¡ Learning rateê°€ ì¤„ì–´ë“¤ì–´ í•™ìŠµì´ ë˜ì§€ ì•Šê²Œ ëœë‹¤.</li>
</ul>

<h3 id="adadelta">Adadelta</h3>

<ul>
  <li><strong>Adadelta</strong> extends Adagrad to reduce its monotonically decreasing the learning rate by restricting the accumulation window.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%208.png" alt="Untitled%208.png" /></p>
<ul class="task-list">
  <li>$0 \leq \gamma \leq 1$</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />EMA = Exponential Moving  Average</li>
</ul>

<h3 id="rmsprop">RMSprop</h3>

<ul>
  <li><strong>RMSprop</strong> is an unpublished, adaptive learning rate method proposed by Geoff Hinton in his lecture.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%209.png" alt="Untitled%209.png" /></p>
<h3 id="adam">Adam</h3>

<ul>
  <li>Adaptive Moment Estimation(<strong>Adam</strong>) leverages both past gradients and squared gradients.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%2010.png" alt="Untitled%2010.png" /></p>
<h2 id="regularization">Regularization</h2>

<h3 id="early-stopping">Early Stopping</h3>

<ul>
  <li>í•™ìŠµì„ ì‹œí‚¤ë‹¤ê°€ lossê°€ ì»¤ì§€ë©´ ê·¸ ë•Œ í•™ìŠµì„ ì¢…ë£Œ ì‹œí‚¤ëŠ” ê²ƒ</li>
  <li>Early Stoppingë¥¼ ìœ„í•œ ì¶”ê°€ì ì¸ validation dataê°€ í•„ìš”í•˜ë‹¤.</li>
</ul>

<h3 id="parameter-norm-penalty">Parameter Norm Penalty</h3>

<ul>
  <li>It adds smoothness to the function space.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%2011.png" alt="Untitled%2011.png" /></p>
<ul>
  <li>Weightê°€ ì‘ìœ¼ë©´ í•¨ìˆ˜ê°€ smoother í•´ì§€ê³  ê·¸ë¬ì„ ê²½ìš°ì—ëŠ” Generalization ì„±ëŠ¥ì´ ë†’ë‹¤ëŠ” ê°€ì •í•˜ì— ì´ ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.</li>
</ul>

<h3 id="data-augmentation">Data Augmentation</h3>

<ul>
  <li>More data are always welcomed.</li>
  <li>However, in most cases, training data are given in advance.</li>
  <li>In such cases we need data augmentation.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%2012.png" alt="Untitled%2012.png" /></p>
<h3 id="noise-robustness">Noise Robustness</h3>

<ul>
  <li>add random noises inputs or weights.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%2013.png" alt="Untitled%2013.png" /></p>
<h3 id="label-smoothing">Label Smoothing</h3>

<ul>
  <li><strong>Mix-up</strong> constructs augmented training exampes by mixing both input and output of two randomly selected training data.</li>
  <li><strong>CutMix</strong> constructs augmented training exaples by mixing inputs with cut and paste and outputs with soft labels of two randomly selected training data.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%2014.png" alt="Untitled%2014.png" /></p>
<h3 id="dropout">Dropout</h3>

<ul>
  <li>In each foward pss, randomly set some neurons to zero.</li>
  <li>ì´ëŠ” ëª¨ë¸ì„ robust í•˜ê²Œ ë§Œë“œëŠ” íš¨ê³¼ê°€ ìˆë‹¤.</li>
</ul>

<h3 id="batch-normalization">Batch Normalization</h3>

<ul>
  <li>Batch normalization compute the empirical mean and variance independently for each dimension (layers) and normalize.</li>
  <li>There are different variances of normalizations.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%2015.png" alt="Untitled%2015.png" /></p>
<h1 id="convolution-neural-network">Convolution Neural Network</h1>

<ul>
  <li>ê¸°ì¡´ì˜ MLPì˜ ê²½ìš°ì—ëŠ” fully connected êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì— ê°€ì¤‘ì¹˜ê°€ ëª¹ì‹œ ì»¤ì§ˆ ìˆ˜ ë°–ì— ì—†ëŠ” êµ¬ì¡°ì˜€ë‹¤.</li>
  <li>Convolution ì—°ì‚°ì€ ì´ì™€ ë‹¬ë¦¬ ì»¤ë„(kernel)ì„ ì…ë ¥ë²¡í„° ìƒì—ì„œ ì›€ì§ì—¬ê°€ë©´ì„œ ì„ í˜•ëª¨ë¸ê³¼ í•©ì„±í•¨ìˆ˜ê°€ ì ìš©ë˜ëŠ” êµ¬ì¡°ì´ë‹¤.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%2016.png" alt="Untitled%2016.png" /></p>
<ul>
  <li>ëª¨ë“  iì— ëŒ€í•´ ì ìš©ë˜ëŠ” ì»¤ë„ì€ Vë¡œ ê°™ê³  ì»¤ë„ì˜ ì‚¬ì´ì¦ˆë§Œí¼ x ìƒì—ì„œ ì´ë™í•˜ë©´ì„œ ì ìš©í•œë‹¤.</li>
  <li>Convolution ì—°ì‚°ì˜ ìˆ˜í•™ì  ì˜ë¯¸ëŠ” ì‹ í˜¸ë¥¼ ì»¤ë„ì„ ì´ìš©í•´ êµ­ì†Œì ìœ¼ë¡œ ì¦í­ ë˜ëŠ” ê°ì†Œì‹œì¼œ ì •ë³´ë¥¼ ì¶”ì¶œ ë˜ëŠ” í•„í„°ë§ í•˜ëŠ” ê²ƒì´ë‹¤.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%2017.png" alt="Untitled%2017.png" />
<img src="/images/2021-02-03/26/Untitled%2018.png" alt="Untitled%2018.png" /></p>
<ul>
  <li>CNNì—ì„œ ì‚¬ìš©ë˜ëŠ” ì—°ì‚°ì€ ì—„ë°€íˆ ë§í•˜ë©´ ëº„ì…ˆì´ ì•„ë‹Œ ë§ì…ˆì´ ì‚¬ìš©ë˜ë¯€ë¡œ cross-correlationì´ì§€ë§Œ, ì—­ì‚¬ì ìœ¼ë¡œ ê³„ì† Convolutionì´ë¼ ë¶ˆë €ìœ¼ë‹ˆ Counvolutionì´ë¼ í•œë‹¤.</li>
  <li>ì»¤ë„ì€ ì •ì˜ì—­ ë‚´ì—ì„œ ì›€ì§ì—¬ë„ ë³€í•˜ì§€ ì•Šê³ (translation invariant) ì£¼ì–´ì§„ ì‹ í˜¸ì— êµ­ì†Œì (local)ìœ¼ë¡œ ì ìš©í•œë‹¤.</li>
</ul>

<h2 id="ë‹¤ì–‘í•œ-ì°¨ì›ì—ì„œì˜-convolution">ë‹¤ì–‘í•œ ì°¨ì›ì—ì„œì˜ Convolution</h2>

<p><img src="/images/2021-02-03/26/Untitled%2019.png" alt="Untitled%2019.png" />
<img src="/images/2021-02-03/26/Untitled%2020.png" alt="Untitled%2020.png" />
<img src="/images/2021-02-03/26/Untitled%2021.png" alt="Untitled%2021.png" /></p>
<ul>
  <li>3ì°¨ì› Convolutionì˜ ê²½ìš°ì—ëŠ” ê° ì±„ë„ë§ˆë‹¤ ì»¤ë„ì´ ì¡´ì¬í•œë‹¤. ê·¸ë˜ì„œ ê° ì±„ë„ì— í•´ë‹¹í•˜ëŠ” ì»¤ë„ì„ ì…ë ¥ì— convolutionì„ í•´ì¤€ ê°’ì„ ì „ë¶€ ë”í•˜ë©´ ëœë‹¤.</li>
</ul>

<p><img src="/images/2021-02-03/26/Untitled%2022.png" alt="Untitled%2022.png" />
<img src="/images/2021-02-03/26/Untitled%2023.png" alt="Untitled%2023.png" /></p>
<ul>
  <li>ì¶œë ¥ì„ 3ì°¨ì›ìœ¼ë¡œ ë§Œë“œë ¤ë©´ ì»¤ë„ì„ ë” ëŠ˜ë ¤ì£¼ë©´ ëœë‹¤.</li>
</ul>

<h2 id="backpropagation-of-convolution">Backpropagation of Convolution</h2>

<p><img src="/images/2021-02-03/26/Untitled%2024.png" alt="Untitled%2024.png" /></p>
:ET