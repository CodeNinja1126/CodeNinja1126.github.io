I";0<h1 id="20일차---self-supervised-pre-training-models">20일차 - Self-supervised Pre-training Models</h1>

<h1 id="gpt-1">GPT-1</h1>

<p><img src="/images/2021-02-20/034/Untitled.png" alt="Untitled.png" />
<img src="/images/2021-02-20/034/Untitled%201.png" alt="Untitled%201.png" /></p>
<ul>
  <li>일반적으로 특수한 목적으로 개발된 모델들 보다 범용적으로 개발된 GPT가 성능이 더 좋다.</li>
  <li>자세한 것은 <a href="https://openai.com/blog/language-unsupervised/">https://openai.com/blog/language-unsupervised/</a></li>
</ul>

<h1 id="bert">BERT</h1>

<p><img src="/images/2021-02-20/034/Untitled%202.png" alt="Untitled%202.png" /></p>
<ul>
  <li>기존의 LSTM을 사용했던 모델들도 결국엔 트랜스포머 모듈로 전부 대체가 되었다.</li>
</ul>

<h2 id="masked-language-modelmlm">Masked Language Model(MLM)</h2>

<ul>
  <li>기존의 언어모델들은 <SOS>토큰을 시작으로 이전의 나왔던 단어들을 이용해 단어를 예측하는 방식으로 학습되었다. 하지만 실제 언어를 이해할 때는 이렇게 한 방향으로만 이해하지 않는다.</SOS></li>
</ul>

<p><img src="/images/2021-02-20/034/Untitled%203.png" alt="Untitled%203.png" /></p>
<ul>
  <li>그래서 몇몇 단어에 임의의 k%로 단어에 마스크를 씌워 단어를 예측하도록 학습시킨다.</li>
  <li>이 때 k가 너무 높으면 단어를 예측하기 쉽지 않고, 너무 낮으면 학습이 더디다.</li>
</ul>

<p><img src="/images/2021-02-20/034/Untitled%204.png" alt="Untitled%204.png" /></p>
<ul>
  <li>하지만 실제 데이터에는 마스크가 없으므로 학습 때와 달리 다르게 동작할 수 있다.</li>
  <li>그래서 k%를 전부 마스크 토큰으로 대체해 주는 것이 아니고, 그 중 10%는 임의의 단어를 입력해 올바르게 고칠 수 있도록 학습시키고, 그 중 10%는 올바른 단어를 입력해 올바른 것도 제대로 판단할 수 있도록 학습시킨다.</li>
</ul>

<h2 id="next-sentence-prediction">Next Sentence Prediction</h2>

<p><img src="/images/2021-02-20/034/Untitled%205.png" alt="Untitled%205.png" /></p>
<ul>
  <li>주어진 두 문장이 이어지는 문장인지 아니면 아무 관련 없는 문장인지 분류할 수 있도록 학습시킨다.</li>
</ul>

<h2 id="input-representation">Input Representation</h2>

<p><img src="/images/2021-02-20/034/Untitled%206.png" alt="Untitled%206.png" /></p>
<ul>
  <li>입력은 각 단어의 token embedding과  문장의 구분하는 segement embedding, 위치를 나타내는 position embedding의 합이다.</li>
</ul>

<h2 id="difference-betweeon-gpt-and-bert">Difference betweeon GPT and BERT</h2>

<p><img src="/images/2021-02-20/034/Untitled%207.png" alt="Untitled%207.png" /></p>
<ul>
  <li>BERT는 모든 단어를 접근 가능하지만 GPT는 이전 시퀀스의 단어만 접근 가능하다.</li>
</ul>

<p><img src="/images/2021-02-20/034/Untitled%208.png" alt="Untitled%208.png" /></p>
<ul>
  <li>BERT는 [SEP], [CLS] 토큰을 사용하였고 segement embedding을 사용하였다.</li>
  <li>BERT는 배치 사이즈가 더 크다. 따라서 학습시키는데 메모리나 GPU의 자원이 더 많이 필요하다.</li>
</ul>

<h2 id="fine-tuning-process">Fine-tuning Process</h2>

<p><img src="/images/2021-02-20/034/Untitled%209.png" alt="Untitled%209.png" /></p>
<ul>
  <li>문장 관계 추론이나 하나의 문장 분류에서는 CLS토큰의 인코딩 벡터를 통해 그 값을 학습 시키게 되고 각 단어에 대한 분류는 각 단어의 인코딩 벡터를 통해 학습시킨다.</li>
</ul>

<h2 id="machine-reading-comprehensionmrc-question-answering">Machine Reading Comprehension(MRC), Question Answering</h2>

<p><img src="/images/2021-02-20/034/Untitled%2010.png" alt="Untitled%2010.png" /></p>
<h3 id="squad-11">SQuAD 1.1</h3>

<p><img src="/images/2021-02-20/034/Untitled%2011.png" alt="Untitled%2011.png" /></p>
<ul>
  <li>SQuAD 1.1은 Question Answering을 위한 데이터 셋이다.</li>
  <li>각 단어의 인코딩 벡터를 fully connected layer를 통해 각 단어에 해당하는 스칼라 값으로 변환하고 이를 소프트맥스에 적용해 답에 해당하는 인덱스의 값이 가장 크도록 학습시킨다.</li>
  <li>이 때 인덱스는 답의 시작에 해당하는 인덱스, 답의 끝에 해당하는 인덱스 총 두 개이다.</li>
</ul>

<h3 id="squad-20">SQuAD 2.0</h3>

<ul>
  <li>SQuAD 2.0은 위의 데이터 셋에 답이 없는 질문을 추가한 데이터 셋이다.</li>
  <li>BERT는 일단 답이 있는지 없는지 CLS 토큰의 인코딩 벡터를 통해 확인하고 없으면 그대로 답이 없다고 반환하고 있다면 위의 답을 찾는 과정을 수행한다.</li>
</ul>

<p><img src="/images/2021-02-20/034/Untitled%2012.png" alt="Untitled%2012.png" /></p>
<h3 id="on-swag">On SWAG</h3>

<ul>
  <li>이어질 문장 중 가장 그럴듯한 문장을 찾는 것</li>
  <li>이 역시 CLS 토큰을 사용하는데, 보기의 각 문장을 이어 보기의 개수 만큼 문장을 만든 후 각 CLS 토큰을 인코딩한 후, 소프트 맥스해서 가장 큰 값을 가진 문장이 답이 되도록 학습한다.</li>
</ul>

<p><img src="/images/2021-02-20/034/Untitled%2013.png" alt="Untitled%2013.png" /></p>
<h2 id="ablation-study">Ablation Study</h2>

<p><img src="/images/2021-02-20/034/Untitled%2014.png" alt="Untitled%2014.png" /></p>
<ul>
  <li>BERT에서는 패러미터를 늘릴수록 성능이 좋아진다. 또한 이는 어느 한계선이 존재하지 않고 늘리면 늘릴수록 계속 좋아졌다.</li>
</ul>

<h1 id="advanced-self-supervised-pre-training-models">Advanced Self-supervised Pre-training Models</h1>

<h2 id="gpt-2">GPT-2</h2>

<ul>
  <li>GPT-2는 정말로 큰 트랜스포머 언어 모델이다.</li>
  <li>40GB의 좋은 퀄리티의 데이터로 학습을 시켰다.</li>
  <li>이 모델은 down-stream tasks in a zero-shot setting(?)를 아무런 패러미터나 구조 변경을 하지 않고 수행할 수 있다.</li>
</ul>

<h3 id="motivation">Motivation</h3>

<ul>
  <li>이것은 모든 태스크들을 Question Answering로 변환될 수 있다는 시각에서 나왔다. (참고 : The Natural Language Decathlon: Multitask Learning as Question Answering)</li>
</ul>

<h3 id="dataset">Dataset</h3>

<ul>
  <li>레딧으로부터 텍스트를 가지고 왔고, 만약 웹페이지 링크였으면 그 웹페이지로부터도 텍스트를 가지고 왔다. (최소 3개 이상의 좋아요를 받은 글)</li>
  <li>800만 개의 제거된 위키피디아 문서들</li>
  <li>드래그넷과 신문을 사용해 링크에서 텍스트를 추출했다.</li>
</ul>

<h3 id="preprocess">Preprocess</h3>

<ul>
  <li>BPE를 사용</li>
  <li>여러 단어 토큰에 대해 단어의 조각화를 최소화 하였다.</li>
</ul>

<h3 id="modification">Modification</h3>

<ul>
  <li>Layer normalization을 각 서브블록의 인풋으로 이동시켰다. 이는 pre-activation residual network(?)과 비슷하다.</li>
  <li>추가적인 Layer normalization이 마지막 self-attention block 이후 추가되었다.</li>
  <li>이 residual layer들의 값이 각 레이어의 깊이에 따라 0에 가깝게 되도록 초기화되었다.</li>
</ul>

<h3 id="question-answering">Question Answering</h3>

<ul>
  <li>CoQA 데이터셋에서 $F_1$점수 55를 달성했다. 이는 높은 점수는 아니지만 어느 정도 가능성을 증명했다.</li>
</ul>

<h3 id="summarization">Summarization</h3>

<ul>
  <li>글 아래에 ‘TL;DR:’(Too long didn’t read) 텍스트를 추가하면 100개의 토큰을 생성해 요약을 수행한다.</li>
</ul>

<h3 id="translation">Translation</h3>

<ul>
  <li>wrote in french와 같이 질문 형태로 텍스트를 주면 그럴 듯한 번역을 수행해 낸다.</li>
</ul>

<h2 id="gpt-3">GPT-3</h2>

<ul>
  <li>역시 기존의 GPT-2에 비해 엄청 커진 모델이다.</li>
  <li>작업에 구애받지 않고, few-shot 성능이 향상되었다.</li>
</ul>

<p><img src="/images/2021-02-20/034/Untitled%2015.png" alt="Untitled%2015.png" /></p>
<ul>
  <li>기존의 모델들이 각 태스크를 수행하기 위해 아웃풋 레이어를 수정하고 fine-tuning을 거쳐야 했지만 GPT-3는 그럴 필요가 없다.</li>
  <li>태스크를 주고 몇 개의 예를 보여주는 것만으로 성능이 크게 향상된다.</li>
  <li>또한 모델이 커질수록 각 zeor-shot 및 few-shot 성능이 향상되었다.</li>
</ul>

<h2 id="albert">ALBERT</h2>

<ul>
  <li>기존의 모델들은 성능을 끌어올리기 위해서는 메모리의 한계와 학습 속도라는 장애물이 있었다.</li>
  <li>하지만 ALBERT는 모델의 사이즈는 더욱더 줄이면서 성능은 향상시켰다.</li>
</ul>

<h3 id="factorized-embedding-parameterization">Factorized Embedding Parameterization</h3>

<p><img src="/images/2021-02-20/034/Untitled%2016.png" alt="Untitled%2016.png" /></p>
<ul>
  <li>embedding dimension을 줄이고 레이어를 하나 더 추가해  E * H 매트릭스로 선형변환을 통해 H크기로 dimension을 다시 맞춰 준다. 이는 패러미터의 수를 상당히 낮출 수 있다.</li>
</ul>

<h3 id="cross-layer-parameter-sharing">Cross-layer Parameter Sharing</h3>

<ul>
  <li>Shared-FFN: feed-foward 네트워크의 패러미터만 공유했을 때</li>
  <li>Shared-FFN: attention 패러미터만 공유했을 때</li>
  <li>위 두가지를 모두 공유하거나 하나만 공유하거나 아예 안 했을 때 모두 그 성능 차이가 크지 않았다.</li>
</ul>

<h3 id="sentence-order-prediction">Sentence Order Prediction</h3>

<ul>
  <li>기존의 BERT에서 수행했던 Next sentence prediction 태스크는 별로 효용성이 없었다.
    <ul>
      <li>NSP는 단지 공통되는 단어를 통해 연관성만을 학습했었다.</li>
    </ul>
  </li>
  <li>그래서 이를 새롭게 Sentence Order Prediction으로 대체해 좀 더 논리적인 흐름을 제대로 잡아낼 수 있도록 하였다.
    <ul>
      <li>SOP는 같은 문단에 순서가 다른 두 문장의 순서를 학습하기 때문에 단순히 두 문장의 연관성만으로는 이를 판단하기 쉽지 않다.</li>
    </ul>
  </li>
</ul>

<h2 id="electra">ELECTRA</h2>

<ul>
  <li>마스크로부터 생성된 대체 단어로부터 진짜로 인풋 토큰과 맞는지 아닌지 구분하는 것을 배운다.</li>
  <li>BERT와 유사한 generator를 통해 단어를 생성하고 discriminator가 이를 진짜인지 아닌지 구분한다.</li>
  <li>이는 GAN구조로 서로 generator과 discriminator이 경쟁적으로 학습한다.</li>
</ul>

<p><img src="/images/2021-02-20/034/Untitled%2017.png" alt="Untitled%2017.png" /></p>
<h2 id="light-weight-models">Light-weight Models</h2>

<h3 id="distillbert">DistillBERT</h3>

<ul>
  <li>teacher모델보다 사이즈가 작은 student 모델이 teacher 모델의 아웃풋을 학습한다.</li>
  <li>teacher 모델의 출력을 결정하는 전체 소프트맥스 값을 학습한다.</li>
</ul>

<h3 id="tinybert">TinyBert</h3>

<ul>
  <li>위의 모델처럼 아웃풋 레이어의 소프트맥스 값 뿐만 아니라, 중간 레이어에서 나타나는 히든 스테이트 백터나, W매트릭스도 학습한다.</li>
  <li>이 때 student 모델은 경량화 모델이므로 벡터의 차원이 다를 수 있는데 이 때 fully connected 레이어를 통해 teacher 모델의 벡터의 차원을 경량화 모델의 차원으로 변환 시키고 이 값으로부터 loss를 추출한다.</li>
  <li>이 과정에서 fully connected 레이어 또한 학습한다.</li>
</ul>

<h2 id="fusing-knowledge-graph-into-language-model">Fusing Knowledge Graph into Language Model</h2>

<p><img src="/images/2021-02-20/034/Untitled%2018.png" alt="Untitled%2018.png" /></p>
<ul>
  <li>일반적으로 BERT와 같은 모델은 문장 내에 존재하는 단어의 연관성이나 문맥은 잘 파악하지만 질문에 답하기 위해서 문장 내에 존재하지 않는 정보가 필요한 경우에는 이를 잘 답하지 못하였다.</li>
  <li>이를 잘 답하기 위해서 지식 정보를 활용하는 모델이 존재한다. 이는 Knowledge graph라는 형태로 모델 안에서 존재하게 된다.</li>
</ul>
:ET