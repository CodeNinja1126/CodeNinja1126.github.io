I"3<h1 id="24일차---정점-표현--추천시스템-활용">24일차 - 정점 표현 &amp; 추천시스템 활용</h1>

<h1 id="정점-표현">정점 표현</h1>

<h2 id="정점-표현-학습">정점 표현 학습</h2>

<ul>
  <li><strong>정점 표현 학습</strong>이란 <strong>그래프의 정점들을 벡터의 형태로 표현하는 것</strong>이다.</li>
  <li>정점 표현 학습은 <strong>정점 임베딩(Node Embedding)</strong>이라고도 부른다. 정점 임베딩은 벡터 형태의 표현 그 자체를 의미하기도 한다. 정점이 표현되는 벡터공간을 임베딩 공간이라 부른다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled.png" alt="Untitled.png" /></p>
<ul>
  <li>정점 임베딩의 결과로, 벡터 형태의 데이터를 위한 도구들을 그래프에 적용할 수 있다.</li>
  <li>기계학습 도구들 (e.g. 분류기(Regression, MLP…), 군집분석 알고리즘(K-Means, DBSCAN)), 딥러닝 도구들을 통해 정점 분류나 군집 분석에 활용할 수 있다.</li>
</ul>

<h3 id="어떻게-정점을-벡터로-변환할-것-인가">어떻게 정점을 벡터로 변환할 것 인가?</h3>

<ul>
  <li>그래프에서의 정점간 유사도를 임베딩 공간에서도 보존하는 것이 목표이다.</li>
  <li>임베딩 공간에서의 유사도로는 내적을 사용한다.</li>
  <li>임베딩 공간에서의 $u$와 $v$의 유사도는 둘의 임베딩의 내적 $z_v^\top z_u = |z_u|\cdot|z_v|\cos(\theta)$이다. 내적은 두 벡터가 클 수록 그리고 같은 방향을 향햘 수록 큰 값을 갖는다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%201.png" alt="Untitled%201.png" /></p>
<ul>
  <li>정점 임베딩은 다음 두 단계로 이루어진다.
    <ol>
      <li>그래프에서의 정점 유사도를 정의하는 단계</li>
      <li>정의나 유사도를 보존하도록 정점 임베딩을 학습하는 단계</li>
    </ol>
  </li>
</ul>

<h2 id="인접성-기반-접근법">인접성 기반 접근법</h2>

<ul>
  <li><strong>인접성(Adjacency) 기반 접근법</strong>에서는 두 정점이 인접할 때 유사하다고 간주한다.</li>
  <li>두 정점 u와 v가 인접하다는 것은 둘을 직접 연결하는 간선 (u, v)가 있음을 의미한다.</li>
  <li>인접행렬(Adjacency Matrix) 𝐀의 𝑢행 𝑣열 원소 $A_{u,v}$는 𝑢와 𝑣가 인접한 경우 1아닌 경우 0이다. 인접행렬의 원소 $A_{u,v}$ 를 두 정점 𝑢와 𝑣의 유사도로 가정한다.</li>
  <li>인접성 기반 접근법의 손실함수는 아래와 같다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%202.png" alt="Untitled%202.png" /></p>
<ul>
  <li>하지만 인접성만으로 유사도를 판단하는 것은 한계가 있다.</li>
  <li>다음 그래프의 경우, 빨간색 정점과 파란색 정점은 군집이 다르므로 실제로 유사도가 적고 인접성 유사도도 0이다. 하지만 파란색 정점과 초록색 정점은 같은 군집으로 볼 수 있지만 인접성 유사도가 0이다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%203.png" alt="Untitled%203.png" /></p>
<h2 id="거리경로중첩-기반-접근법">거리/경로/중첩 기반 접근법</h2>

<h3 id="거리-기반-접근법">거리 기반 접근법</h3>

<ul>
  <li>거리 기반 접근법에서는 두 정점 사이의 거리가 충분히 가까운 경우 유사하다고 간주한다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%204.png" alt="Untitled%204.png" /></p>
<h3 id="경로-기반-접근법">경로 기반 접근법</h3>

<ul>
  <li>경로 기반 접근법에서는 두 정점 사이의 경로가 많을 수록 유사하다고 간주한다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%205.png" alt="Untitled%205.png" /></p>
<h3 id="중첩-기반-접근법">중첩 기반 접근법</h3>

<ul>
  <li>중첩 기반 접근법에서는 두 정점이 많은 이웃을 공유할 수록 유사하다고 간주한다.</li>
  <li>아래 그림에서 빨간색 정점은 파란색 정점과 두 명의 이웃을 공유하기 때문에 유사도는 2가 된다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%206.png" alt="Untitled%206.png" />
<img src="/images/2021-02-27/038/Untitled%207.png" alt="Untitled%207.png" /></p>
<ul>
  <li>공통 이웃 수 대신, <strong>자카드 유사도</strong> 혹은 <strong>Adamic Adar점수</strong>를 사용할 수도 있다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%208.png" alt="Untitled%208.png" /></p>
<ul>
  <li>이 때 Adamic Adar점수를 사용할 때 해당 이웃의 연결성을 나누어주는 이유는 해당 이웃의 연결성이 높을 경우 해당 이웃을 공유하는 두 정점의 유사도가 높지 않을 가능성이 높기 때문이다.
    <ul>
      <li>예를 들어, 두 사람이 아이돌의 계정을 팔로우했을 경우 두 사람은 관련이 없을 가능성이 높다. 반면 두 사람이 팔로우 수가 적은 사람을 팔로우 했을 경우, 이 두 사람은 관련이 있는 사람일 가능성이 높다.</li>
    </ul>
  </li>
</ul>

<h2 id="임의보행-기반-접근법">임의보행 기반 접근법</h2>

<ul>
  <li>임의보행 기반 접근법에서는 한 정점에서 시작하여 임의보행을 할 때, 다른 정점에 도달할 확률을 유사도로 간주한다.</li>
  <li>임의 보행이란 현재 정점의 이웃 중 하나를 균일한 확률로 선택하는 이동하는 과정을 반복하는 것을 의미한다.</li>
  <li>임의 보행을 사용할 경우 시작 정점 주변의 지역적 정보와 그래프 전역 정보를 모두 고려한다는 장점이 있다.</li>
  <li>임의 보행 기반 접근법은 다음 세 단계를 거친다.
    <ol>
      <li>각 정점에서 시작하여 임의보행을 반복 수행한다.</li>
      <li>각 정점에서 시작한 임의보행 중 도달한 정점들의 리스트를 구성한다. 이 때, 정점 $u$에서 시작한 임의 보행 중 도달한 정점들의 리스트를 $N_R(u)$라고 합시다. 한 정점을 여러 번 도달한 경우, 해당 정점은 $N_R(u)$에 여러 번 포함될 수 있다.</li>
      <li>다음 손실함수를 최소화하는 임베딩을 학습한다.</li>
    </ol>
  </li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%209.png" alt="Untitled%209.png" /></p>
<ul>
  <li>임베딩으로부터 도달 확률을 추정하는 방법은 소프트 맥스이다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2010.png" alt="Untitled%2010.png" /></p>
<ul>
  <li>이 때 소프트 맥스 함수를 사용하는 이유는 $z_u, z_v$의 유사도가 높기를 원하기 때문이다. 즉, $P(v\mid z_u)$가 1에 가까울 수록 $z_u, z_v$의 유사도는 커질 것이다.</li>
  <li>손실 함수는 다음과 같다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2011.png" alt="Untitled%2011.png" /></p>
<ul>
  <li>이렇게 일반적인 임의보행의 방법을 따르는 것을 Deepwalk라고 한다.</li>
</ul>

<h3 id="node2vec">Node2Vec</h3>

<ul>
  <li>Node2Vec은 2차 치우친 임의보행(Second-order Biased Random Walk)을 사용한다.</li>
  <li>현재 정점(예시에서 𝑣)과 직전에 머물렀던 정점(예시에서 𝑢)을 모두 고려하여 다음 정점을 선택한다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2012.png" alt="Untitled%2012.png" />
<img src="/images/2021-02-27/038/Untitled%2013.png" alt="Untitled%2013.png" /></p>
<ul>
  <li>멀어지는 방향에 높은 확률을 부여한 경우, 정점의 역할(다리 역할, 변두리 정점 등)이 같은 경우 임베딩이 유사하다.</li>
  <li>가까워지는 방향에 높은 확률을 부여한 경우, 같은 군집에 속한 경우 임베딩이 유사하다.</li>
</ul>

<h3 id="손실-함수-근사">손실 함수 근사</h3>

<ul>
  <li>임의 보행 기법의 경우에는 계산에 정점의 수의 제곱에 비례하는 시간이 소요된다. 따라서 근사식을 이용한다.</li>
  <li>모든 정점에 대해서 정규화하는 대신 몇 개의 정점을 뽑아서 비교한다. 이 때 뽑힌 정점들을 네거티브 샘플이라고 부른다.</li>
  <li>네거티브 샘플의 수가 많을수록 더욱 안정적인 학습을 할 수 있다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2014.png" alt="Untitled%2014.png" /></p>
<h2 id="변환식-정점-표현-학습의-한계">변환식 정점 표현 학습의 한계</h2>

<ul>
  <li>지금까지의 정점 임베딩 방법들은 <strong>변환식(Transductive)</strong>방법이다.</li>
  <li>변환식(Transductive)방법은 학습의 결과로 정점의 임베딩 자체를 얻는다는 특성이 있다. 정점을 임베딩으로 변화시키는 함수, 즉 인코더를 얻는 귀납식(Inductive) 방법과 대조된다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2015.png" alt="Untitled%2015.png" /></p>
<ul>
  <li>변환식 임베딩 방법의 한계는 다음과 같다.
    <ol>
      <li>학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없다.</li>
      <li>모든 정점에 대해 임베딩을 미리 계산하여 저장해두어야 한다.</li>
      <li>정점이 속성(Attribute) 정보를 가진 경우에 이를 활용할 수 없다.</li>
    </ol>
  </li>
  <li>대표적인 귀납식 임베딩 방법으로는 GNN(Graph NN)이 있다.</li>
</ul>

<h1 id="추천-시스템-활용">추천 시스템 활용</h1>

<h2 id="넷플릭스-챌린지">넷플릭스 챌린지</h2>

<ul>
  <li>넷플릭스 챌린지 에서는 사용자별 영화 평점 데이터가 사용되었다.</li>
  <li>훈련 데이터(Training Data)는 2000년부터 2005년까지 수집한 48만명 사용자의 1만 8천개의 영화에 대한 1억 개의 평점으로 구성되어 있다. 평가 데이터(Test Data)는 각 사용자의 최신 평점 280만개로 구성되어 있다.</li>
  <li>목표는 추천 시스템의 성능을 10%이상 향상시키는 것이었다. 넷플릭스 챌린지를 통해 추천시스템의 성능이 비약적으로 발전했다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2016.png" alt="Untitled%2016.png" /></p>
<h2 id="잠재-인수-모형latent-factor-model">잠재 인수 모형(Latent Factor Model)</h2>

<ul>
  <li>잠재 인수 모형의 핵심은 사용자와 상품을 벡터로 표현하는 것이다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2017.png" alt="Untitled%2017.png" />
<img src="/images/2021-02-27/038/Untitled%2018.png" alt="Untitled%2018.png" /></p>
<ul>
  <li>사용자와 상품을 임베딩하는 기준은 사용자와 상품의 임베딩의 내적이 평점과 최대한 유사하도록 하는 것이다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2019.png" alt="Untitled%2019.png" /></p>
<ul>
  <li>잠재 인수 모형은 다음 손실함수를 최소화하는 $P$와 $Q$를 찾는 것을 목표로 한다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2020.png" alt="Untitled%2020.png" /></p>
<ul>
  <li>하지만 위의 손실 함수를 그대로 사용할 경우, Overfitting이 발생할 수 있다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2021.png" alt="Untitled%2021.png" /></p>
<ul>
  <li>그래서 위와 같이 정규화 항을 추가해 Overfitting을 방지한다.</li>
</ul>

<h2 id="고급-잠재-인수-모형">고급 잠재 인수 모형</h2>

<h3 id="사용자와-상품의-편향을-고려한-잠재-인수-모형">사용자와 상품의 편향을 고려한 잠재 인수 모형</h3>

<ul>
  <li>각 <strong>사용자의 편향</strong>은 <strong>해당 사용자의 평점 평균과 전체 평점 평균의 차</strong>이다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2022.png" alt="Untitled%2022.png" /></p>
<ul>
  <li>각 <strong>상품의 편향</strong>은 <strong>해당 상품의 평점 평균과 전체 평점 평균의 차</strong>이다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2023.png" alt="Untitled%2023.png" />
<img src="/images/2021-02-27/038/Untitled%2024.png" alt="Untitled%2024.png" />
<img src="/images/2021-02-27/038/Untitled%2025.png" alt="Untitled%2025.png" /></p>
<ul>
  <li>경사하강법을 통해 손실 함수를 최소화하는 편향과 잠재 인수를 찾아낸다.</li>
</ul>

<h3 id="시간적-편향을-고려한-잠재-인수-모형">시간적 편향을 고려한 잠재 인수 모형</h3>

<ul>
  <li>일반적으로 영화의 평점은 출시일 이후 시간이 지만에 따라 상승하는 경향을 갖는다.</li>
</ul>

<p><img src="/images/2021-02-27/038/Untitled%2026.png" alt="Untitled%2026.png" />
<img src="/images/2021-02-27/038/Untitled%2027.png" alt="Untitled%2027.png" /></p>
<ul>
  <li>이렇게 시간적 편향까지 고려해 오차는 0.876까지 줄어들게 된다.</li>
</ul>

<h2 id="넷플릭스-챌린지의-결과">넷플릭스 챌린지의 결과</h2>

<h3 id="앙상블-학습">앙상블 학습</h3>

<ul>
  <li>결과적으로 다양한 모델을 이용한 앙상블 학습을 사용해 처음으로 목표 성능에 도달하였다.</li>
</ul>
:ET