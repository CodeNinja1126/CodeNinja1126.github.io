I"}$<h1 id="경사하강법">경사하강법</h1>

<h2 id="미분">미분</h2>

<ul>
  <li>미분(differentiation)은 변수의 움직임에 따른 <strong>함수값의 변화를 측정하기 위한 도구로 최적화에서 제일 많이 사용하는 기법</strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sympy</span> <span class="k">as</span> <span class="n">sym</span>
<span class="kn">from</span> <span class="nn">sympy.abc</span> <span class="kn">import</span> <span class="n">x</span>

<span class="n">sym</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">sym</span><span class="p">.</span><span class="n">poly</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
<span class="c1"># Poly(2*x + 2, x, domain='ZZ')
</span></code></pre></div></div>

<p><img src="/images/2021-01-27/1.png" alt="1.png" /></p>

<ul>
  <li>접선의 기울기를 알면, 어느 방향으로 점을 움직이면 함수값이 증가하는 지 감소하는 지 알수 있다.</li>
</ul>

<p><img src="/images/2021-01-27/2.png" alt="2.png" /></p>

<p><img src="/images/2021-01-27/3.png" alt="3.png" /></p>

<ul>
  <li><strong>경사상승</strong>/<strong>경사하강</strong> 방법은 극값에 도달하면 움직임을 멈춘다.</li>
</ul>

<h2 id="경사하강법-알고리즘">경사하강법: 알고리즘</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">var</span> <span class="o">=</span> <span class="n">init</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
<span class="k">while</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">gra</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>

<span class="c1"># gradient: 미분을 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건
</span></code></pre></div></div>

<ul>
  <li>컴퓨터로는 미분이 정확하게 0이 되도록 계산하는 것은 불가능하다. 따라서 eps를 설정해 종료조건을 설정해 주는 것</li>
  <li>lr값을 통해 경사를 내려가는 속도를 제어할 수 있다. 이 값은 주의해서 다뤄야 한다.</li>
</ul>

<h2 id="변수가-벡터라면">변수가 벡터라면?</h2>

<ul>
  <li>미분은 <strong>변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구</strong>이다.</li>
</ul>

<p><img src="/images/2021-01-27/4.png" alt="4.png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sympy</span> <span class="k">as</span> <span class="n">sym</span>
<span class="kn">from</span> <span class="nn">sympy.abc</span> <span class="kn">import</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

<span class="n">sym</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">sym</span><span class="p">.</span><span class="n">poly</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">y</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="n">sym</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">y</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
<span class="c1">#2𝑥+2𝑦−sin(𝑥+2𝑦)
</span></code></pre></div></div>

<h2 id="그레디언트-벡터">그레디언트 벡터</h2>

<p><img src="/images/2021-01-27/5.png" alt="5.png" /></p>

<p><img src="/images/2021-01-27/6.png" alt="6.png" /></p>
<h2 id="경사하강법-알고리즘-1">경사하강법: 알고리즘</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">var</span> <span class="o">=</span> <span class="n">init</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
<span class="k">while</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>

<span class="c1"># gradient: 그레디언트 벡터를 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건
</span></code></pre></div></div>

<h1 id="경사하강법-선형회귀-계수-구하기">경사하강법: 선형회귀 계수 구하기</h1>

<p><img src="/images/2021-01-27/7.png" alt="7.png" /></p>

<p><img src="/images/2021-01-27/8.jpg" alt="8.jpg" /></p>

<h2 id="경사하강법-알고리즘-2">경사하강법 알고리즘</h2>

<ul>
  <li>
    <p>목적식을 최소화하는 $\beta$을 최소화하는 경사하강법 알고리즘</p>
  </li>
  <li>
    <p>$\beta^{(t+1)}\leftarrow\beta^{(t)}-\lambda\nabla_\beta|y - X\beta^{(t)}|$</p>
  </li>
  <li>
    <p>$\beta^{(t+1)}\leftarrow\beta^{(t)}-\dfrac\lambda n\dfrac{X^T(y-X\beta^{(t)})}{|y - X\beta^{(t)}|}$</p>
  </li>
  <li>
    <p>L-2 노름의 제곱을 최소화하여도 그 목적은 같다고 할 수 있다.</p>
  </li>
</ul>

<p><img src="/images/2021-01-27/9.png" alt="9.png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">x</span> <span class="o">@</span> <span class="n">Beta</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="o">-</span> <span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">error</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>

<span class="c1"># @는 행렬곱
# 이 방식은 종료조건을 학습횟수로 지정한 방식이다.
</span></code></pre></div></div>

<ul>
  <li>이론적으로 경사하강법은 미분가능하고 볼록한 함수에 대해선 <strong>적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장</strong>된다.</li>
  <li>특히 선형회귀의 경우 목적식 $|y - X\beta|_2$는 <strong>희귀계수 $\beta$에 대해 볼록함수</strong>이기 때문에 알고리즘을 충분히 돌리면 수렴이 보장됨</li>
  <li>하지만 <strong>비선형회귀</strong> 문제의 경우 목적식이 볼록하지 않을 수 있으므로 <strong>수렴이 항상 보장되지는 않는다.</strong></li>
</ul>

<h2 id="확률적-경사하강법">확률적 경사하강법</h2>

<ul>
  <li><strong>확률적 경사하강법(stochastic gradient descent)</strong>는 모든 데이터를 사용해서 업데이트하는 대신 데이터 한개 또는 일부 활용하여 업데이트한다.</li>
  <li>볼록이 아닌(non-convex) 목적식은 SGD를 통해 최적화할 수 있다.</li>
  <li>데이터를 한 개만 사용하면 그냥 SGD, 일부만 사용하면 mini-batch SGD라고 한다.</li>
  <li>이 역시 만능은 아니지만 딥러닝의 경우 SGD가 경사하강법보다 실증적으로 더 낫다고 한다.</li>
</ul>

<p><img src="/images/2021-01-27/10.png" alt="10.png" /></p>

<p><img src="/images/2021-01-27/11.png" alt="11.png" /></p>

<ul>
  <li>배치를 사용할 때마다 곡선 모양이 바뀌므로 극소점을 탈출할 가능성이 생긴다.</li>
</ul>
:ET