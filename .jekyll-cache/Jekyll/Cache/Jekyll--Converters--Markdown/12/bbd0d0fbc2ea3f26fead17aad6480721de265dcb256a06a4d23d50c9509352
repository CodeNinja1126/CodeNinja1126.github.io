I"!<h1 id="25일차---gnn">25일차 - GNN</h1>

<h1 id="그래프-신경망-기본">그래프 신경망 기본</h1>

<h1 id="그래프-신경망-구조">그래프 신경망 구조</h1>

<ul>
  <li>그래프 신경망은 <strong>그래프</strong>와 <strong>정점의 속성 정보</strong>를 입력을 받는다.</li>
  <li>그래프 입력은 <strong>그래프의 인접 행렬</strong> 이다.</li>
  <li>속성정보의 입력은 정점이 $u$라고 했을 때, 속성(Attribute) 벡터 $X_u$이다. 이때 속성 벡터 $X_u$는 $m$차원 벡터이고, $m$은 속성의 수를 의미한다.</li>
  <li>정점의 속성의 예로는 다음과 같은 것이 있다.
    <ul>
      <li>온라인 소셜 네트워크에서의 사용자의 지역, 성별, 연령 등</li>
      <li>논문 인용 그래프에서 논문에 사용된 키워드에 대한 원-핫 벡터</li>
      <li>PageRank 등의 정점 중심성, 군집 계수(Clustering Coefficient) 등</li>
    </ul>
  </li>
</ul>

<p><img src="2021-03-04/039/Untitled.png" alt="Untitled.png" /></p>
<ul>
  <li>그래프 신경망은 이웃 정점들의 정보를 집계하는 과정을 반복하여 임베딩을 얻는다.</li>
  <li>대상 정점의 임베딩을 얻기 위해 이웃들 그리고 이웃의 이웃들의 정보를 집계한다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%201.png" alt="Untitled%201.png" />
<img src="2021-03-04/039/Untitled%202.png" alt="Untitled%202.png" /></p>
<ul>
  <li>다른 대상 정점 간에도 층 별 집계함수는 <strong>공유</strong>한다.</li>
  <li>그렇다면 이렇게 입력이 다른데 어떻게 함수를 공유할 수 있을까? 방법은 평균이다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%203.png" alt="Untitled%203.png" /></p>
<ul>
  <li>마지막 층에서의 정점 별 임베딩이 해당 정점의 출력 임베딩이다.</li>
</ul>

<h2 id="그래프-신경망-학습">그래프 신경망 학습</h2>

<p><img src="2021-03-04/039/Untitled%204.png" alt="Untitled%204.png" /></p>
<ul>
  <li>일단 손실함수를 결정한다. 정점간 거리를 ‘보존’하는 것을 목표로 할 수 있다. 인접성을 기반으로 그래프의 정점 간 유사도를 결정한다면, 손실함수를 다음과 같이 정의할 수 있다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%205.png" alt="Untitled%205.png" /></p>
<ul>
  <li>후속 과제(Downstream Task)의 손실함수를 이용한 종단종(End-to-End)학습도 가능하다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%206.png" alt="Untitled%206.png" />
<img src="2021-03-04/039/Untitled%207.png" alt="Untitled%207.png" /></p>
<ul>
  <li>그래프 신경망의 종단종 학습을 통한 분류는 변환적 정점 임베딩 이후에 별도의 분류기를 학습하는 것보다 정확도가 대체로 높다.</li>
  <li>학습된 신경망을 적용하여, 학습에 사용되지 않은 정점의 임베딩을 얻을 수 있다.</li>
  <li>마찬가지로 학습 이후에 추가된 정점의 임베딩도 얻을 수 있으며, 학습된 그래프 신경망을, 새로운 그래프에 적용할 수도 있다.</li>
</ul>

<h1 id="그래프-신경망-변형">그래프 신경망 변형</h1>

<h2 id="합성곱-신경망">합성곱 신경망</h2>

<ul>
  <li>위의 기본적인 그래프 신경망 말고도 다른 집계함수를 사용할 수 있다.</li>
  <li>바로 그래프 합성곱 신경망(Graph Convolutional Network, GCN)의 집계 함수이다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%208.png" alt="Untitled%208.png" /></p>
<ul>
  <li>GCN은 정규화에 기하평균을 사용하며, 학습 변수 $B_k$를 사용하지 안흔ㄴ다.</li>
</ul>

<h2 id="graphsage">GraphSAGE</h2>

<ul>
  <li>GraphSAGE의 집계함수는 다음과 같다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%209.png" alt="Untitled%209.png" />
<img src="2021-03-04/039/Untitled%2010.png" alt="Untitled%2010.png" /></p>
<ul>
  <li>이 때, LSTM에 사용되는 $\pi()$함수는 정점 $v$의 이웃의 순서를 무작위로 섞는다는 의미로, 순서를 섞어 LSTM에 입력시켜주는 의미가 있다.</li>
</ul>

<h1 id="합성곱-신경망cnn과의-비교">합성곱 신경망(CNN)과의 비교</h1>

<ul>
  <li>합성곱 신경망과 그래프 신경망은 모두 이웃의 정보를 집계하는 과정을 반복한다.</li>
  <li>CNN과 GNN의 가장 큰 차이점은 CNN은 매번 합성곱을 하는 컨볼루션 마스크의 크기가 항상 일정하지만 그래프의 경우에는 이웃의 수가 정점 별로 다르다는 것이 가장 큰 차이점이다.</li>
  <li>그러면 그래프의 인접 행렬에 CNN을 사용하면 효과적이지 않을까?</li>
  <li>답은 그렇지 않다이다. 일반적으로 CNN이 주로 사용되는 이미지에서는 인접하는 픽셀이 유용한 정보를 담고 있을 가능성이 높다. 하지만 그래프의 인접 행렬에서는 어떠한 값과 인접하고 있는 값이 서로 아무 관련이 없는 정보일 가능성이 높기 때문에 인접행렬에서의 CNN은 그다지 효과적이지 않다.</li>
</ul>

<h1 id="그래프-신경망-심화">그래프 신경망 심화</h1>

<h1 id="그래프-신경망에서의-어탠션">그래프 신경망에서의 어탠션</h1>

<ul>
  <li>기본 그래프 신경망에서는 한계가 있다. 바로 정점의 이웃들의 정보를 동일한 가중치로 평균을 낸다는 것이다.</li>
  <li>그래프 어탠션 신경망(GAT)에서는 가중치 자체도 학습한다. 실제 그래프에서는 이웃 별로 미치는 영향이 다를 수 있기때문이다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%2011.png" alt="Untitled%2011.png" /></p>
<ul>
  <li>어탠션의 계산은 다음과 같다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%2012.png" alt="Untitled%2012.png" /></p>
<ul>
  <li>멀티 헤드 어탠션 또한 존재한다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%2013.png" alt="Untitled%2013.png" /></p>
<h1 id="그래프-표현-학습">그래프 표현 학습</h1>

<ul>
  <li>그래프 표현 학습, 혹은 그래프 임베딩이란 그래프 전체를 벡터의 형태로 표현하는 것이다.</li>
  <li>그래프 임베딩은 정점 하나를 벡터로 표현하는 정점 표현 학습과는 다르다. 그래프 자체를 임베딩해 그래프 자체의 특성을 예측하는데 활용된다.</li>
  <li>그래프 풀링(Graph Pooling)이란 정점 임베딩들로부터 그래프 임베딩을 얻는 과정이다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%2014.png" alt="Untitled%2014.png" /></p>
<h1 id="지나친-획일화-문제">지나친 획일화 문제</h1>

<ul>
  <li>지나친 획일화(Over-smoothing) 문제란 그래프 신경망의 층의 수가 증가하면서 정점의 임베딩이 서로 유사해지는 현상이다.</li>
  <li>지나친 획일화 문제는 작은 세상 효과와 관련이 있다. 즉 신경망의 층이 깊어질수록 반영하는 정점의 범위가 넓어지고, 결과적으로는 모든 정점이 비슷한 형태를 띄게된다.</li>
  <li>이에 대한 대비책으로는 잔차항(Residual)(이전 층의 임베딩)을 넣어 주는 것이다. 하지만 이것만으로는 효과가 제한적이다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%2015.png" alt="Untitled%2015.png" /></p>
<ul>
  <li>그래서 JK네트워크(Jumping Knowledge Network)는 마지막 층의 임베딩 뿐 아니라, 모든 층의 임베딩을 함께 사용한다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%2016.png" alt="Untitled%2016.png" /></p>
<ul>
  <li>APPNP는 0번째 층을 제외하고는 신경망 없이 집계 함수를 단순화시켰다.</li>
</ul>

<p><img src="2021-03-04/039/Untitled%2017.png" alt="Untitled%2017.png" /></p>
<ul>
  <li>APPNP의 경우, 층의 수 증가에 따른 정확도 감소 효과가 없는 것을 확인하였다.</li>
</ul>

<h1 id="그래프-데이터-증강">그래프 데이터 증강</h1>

<ul>
  <li>데이터 증강(Data Augmentation)은 다양한 기계학습 문제에서 효과적이다.</li>
  <li>그래프에서도 누락되거나 부정확한 간선이 있을 수 있고, 데이터 증강을 통해 이를 보완할 수 있다.</li>
  <li>임의 보행을 통해 정점간 유사도를 계산하고 유사도가 높은 정점 간의 간선을 추가하는 방법이 있다.</li>
  <li>그래프 데이터 증강의 결과 정점 분류의 정확도가 개선되었다.</li>
</ul>
:ET