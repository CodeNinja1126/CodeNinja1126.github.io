I"<h1 id="16일차---nlp-기본">16일차 - NLP 기본</h1>

<h1 id="nlpnatural-language-processing">NLP(Natural language processing)</h1>

<ul>
  <li>NLP에는 최첨단 딥러닝 모델과 함께 그 외에 다음과 같은 작업들을 포함한다.</li>
  <li>Low-level parsing
• Tokenization, stemming</li>
  <li>Word and phrase level
• Named entity recognition(NER), part-of-speech(POS) tagging, noun-phrase chunking, dependency parsing, coreference resolution</li>
  <li>Sentence level
• Sentiment analysis, machine translation</li>
  <li>Multi-sentence and paragraph level
• Entailment prediction, question answering, dialog systems, summarization</li>
</ul>

<h2 id="nlp-학문-분야">NLP 학문 분야</h2>

<h3 id="text-mining">Text mining</h3>

<ul>
  <li>주요 컨퍼런스로는 KDD, The WebConf (formerly, WWW), WSDM, CIKM, ICWSM이 있다.</li>
  <li>문서로부터 유용한 정보와 시각을 추출한다.</li>
  <li>Document clustering</li>
  <li>소셜 과학과 깊게 연관되어 있다.</li>
</ul>

<h3 id="information-retrieval">Information retrieval</h3>

<ul>
  <li>주요 컨퍼런스로는 SIGIR, WSDM, CIKM, RecSys가 있다.</li>
  <li>정보 검색, 이 역시 소셜 과학과 깊게 연관되어 있다.</li>
  <li>검색은 어느 정도 성숙해져 있는 분야라 성장이 더디지만 최근 추천 시스템을 통해 새로운 길이 열리는 추세이다.</li>
</ul>

<h2 id="nlp-트렌드">NLP 트렌드</h2>

<ul>
  <li>텍스트는 기본적으로 단어의 시퀀스 데이터이다. 각 단어는 Word2Vec 이나 GloVe와 같은 기술들로 벡터로 변환될 수 있다.</li>
  <li>RNN 모델들은 이러한 시퀀스 벡터들을 입력으로 받아 처리하는 NLP의 주요 모델들이다.</li>
  <li>어탠션과 트랜스포머의 등장으로 NLP는 비약적으로 성능이 향상되었다.</li>
  <li>트랜스포머와 마찬가지로 고급 NLP들은 원래 기계 번역 작업을 위해 개발되었다.</li>
  <li>초기에는 NLP 작업마다 따로 모델들이 설계되었다. 하지만 트랜스포머가 소개된 이후로는 특정 작업에 필요한 추가적인 레이블링을 할 필요가 없어졌다. ex) BERT, GPT-3</li>
  <li>게다가 이러한 모델들은 특정 작업에 특화된 전용 모델보다 높은 성능을 보여주었다.</li>
  <li>현재에는 이러한 모델들이 수많은 NLP 작업에서 필수적인 부분이 되었다. 하지만 이러한 모델은 많은 양의 데이터로 학습을 시켜야 하기 때문에 자원이 많이 든다.</li>
  <li>따라서 오늘날 NLP 연구는 거대 자본이 이끌게 되었다.</li>
</ul>

<h1 id="bag-of-words">Bag-of-Words</h1>

<h2 id="bag-fo-words">Bag-fo-Words</h2>

<ol>
  <li>유니크한 단어들로 이루어진 단어집을 형성한다.
    <ul>
      <li>Example sentences: “John really really loves this movie“, “Jane really likes this song”</li>
      <li>Vocabulary: {“John“, “really“, “loves“, “this“, “movie“, “Jane“, “likes“, “song”}</li>
    </ul>
  </li>
  <li>유니크 단어들을 원핫 벡터로 표현한다.
    <ul>
      <li>John: [1 0 0 0 0 0 0 0]</li>
      <li>really: [0 1 0 0 0 0 0 0]</li>
      <li>loves: [0 0 1 0 0 0 0 0]</li>
      <li>this: [0 0 0 1 0 0 0 0]</li>
      <li>movie: [0 0 0 0 1 0 0 0]</li>
      <li>Jane: [0 0 0 0 0 1 0 0]</li>
      <li>likes: [0 0 0 0 0 0 1 0]</li>
      <li>song: [0 0 0 0 0 0 0 1]</li>
      <li>이 단어들의 짝은 모두 거리가 $\sqrt {2}$이다. 또한 코사인 유사도가 0이다.</li>
    </ul>
  </li>
  <li>문장은 이 벡터들의 합으로 표현해줄 수 있다.
    <ul>
      <li>Sentence 1: “John really really loves this movie“
        <ul>
          <li>John + really + really + loves + this + movie: [1 2 1 1 1 0 0 0]</li>
        </ul>
      </li>
      <li>Sentence 2: “Jane really likes this song”
        <ul>
          <li>Jane + really + likes + this + song: [0 1 0 1 0 1 1 1]</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="naivebayes-classifier">NaiveBayes Classifier</h2>

<ul>
  <li>베이즈 법칙을 통해 문서를 분류하기</li>
</ul>

<p><img src="/images/2021-02-16/030/Untitled.png" alt="Untitled.png" /></p>
<ul>
  <li>클래스중 가장 $P(c\mid d)$를 크게하는 클래스가 해당 문서의 클래스일 것이다.</li>
  <li>$P(d\mid c)P(c) = P(w_1,w_2,…,w_n\mid c) \rightarrow P(c)\prod_{w_i \in W} P(w_i \mid c)$</li>
  <li>조건부 독립이라는 가정하에 위의 식이 성립한다.</li>
</ul>

<p><img src="/images/2021-02-16/030/Untitled%201.png" alt="Untitled%201.png" />
<img src="/images/2021-02-16/030/Untitled%202.png" alt="Untitled%202.png" /></p>
<ul>
  <li>$P(c_{NLP}\mid d_5)$가 더 크므로 문서는 NLP 클래스로 분류된다는 사실을 알 수 있다.</li>
</ul>

<h1 id="word-embedding">Word Embedding</h1>

<ul>
  <li>단어를 벡터로 표현하는 것이다.</li>
  <li>예를 들어 ‘cat’ 와 ‘kitty’는 비슷한 단어이므로, 그들은 비슷한 벡터 형태를 가진다. 즉 두 벡터의 거리는 짧을 것이다.</li>
  <li>하지만 ‘hamburger’은 위의 두 단어와 비슷하지 않다. 따라서 그들은 다른 벡터 형태를 가질 것이다. 즉 두 벡터의 거리는 멀 것이다.</li>
  <li>한 단어의 뜻은 그 단어와 함께 나오는 단어들의 분포로 근사할 수 있다.</li>
</ul>

<h2 id="word2vec">Word2Vec</h2>

<p><img src="/images/2021-02-16/030/Untitled%203.png" alt="Untitled%203.png" /></p>
<ul>
  <li>슬라이딩 윈도우를 이용해 같이 나오는 단어들을 각각 짝으로 맞춘다. 예를 들어 크기 3짜리 윈도우라면 ‘study’의 경우, 앞 뒤로 (‘study’, ‘I’), (‘study’, ‘math’) 두 개의 짝이 나올 것이다.</li>
  <li>윈도우를 통해 나온 짝을 2층 신경망의 입출력으로 학습시킨다.</li>
  <li>이 때 히든 레이어의 노드 개수는 하이퍼 파라미터이다.</li>
  <li>$W_1$의 column의 수와 $W_2$의 row의 수는 총 단어의 개수이다. 각각의 column과 row는 각 단어의 벡터를 의미한다.</li>
  <li>일반적으로 단어 벡터는 $W_1$을 사용하게 된다.</li>
</ul>

<h2 id="glove-global-vectors-for-word-representation">GloVe: Global Vectors for word Representation</h2>

<ul>
  <li>동시에 나오는 단어들을 미리 계산해서 두번 이상 나오는 짝에 대해서 여러번 학습시킬 필요가 없다.</li>
  <li>학습이 빠르고 작은 말뭉치에서도 잘 동작한다.</li>
  <li>다음의 로스함수를 이용해 학습시킨다. 이 때, $P_{ij}$는 한 윈도우 내에서 두 단어가 동시 발생한 횟수이다.</li>
</ul>

<p><img src="/images/2021-02-16/030/Untitled%204.png" alt="Untitled%204.png" /></p>
:ET