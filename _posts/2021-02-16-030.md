---
title: 16일차 - NLP 기본
tags: [boostcamp]
published: true
use_math: true
comments: true
---

# 16일차 - NLP 기본

# NLP(Natural language processing)

- NLP에는 최첨단 딥러닝 모델과 함께 그 외에 다음과 같은 작업들을 포함한다.
- Low-level parsing
• Tokenization, stemming
- Word and phrase level
• Named entity recognition(NER), part-of-speech(POS) tagging, noun-phrase chunking, dependency parsing, coreference resolution
- Sentence level
• Sentiment analysis, machine translation
- Multi-sentence and paragraph level
• Entailment prediction, question answering, dialog systems, summarization

## NLP 학문 분야

### Text mining

- 주요 컨퍼런스로는 KDD, The WebConf (formerly, WWW), WSDM, CIKM, ICWSM이 있다.
- 문서로부터 유용한 정보와 시각을 추출한다.
- Document clustering
- 소셜 과학과 깊게 연관되어 있다.

### Information retrieval

- 주요 컨퍼런스로는 SIGIR, WSDM, CIKM, RecSys가 있다.
- 정보 검색, 이 역시 소셜 과학과 깊게 연관되어 있다.
- 검색은 어느 정도 성숙해져 있는 분야라 성장이 더디지만 최근 추천 시스템을 통해 새로운 길이 열리는 추세이다.

## NLP 트렌드

- 텍스트는 기본적으로 단어의 시퀀스 데이터이다. 각 단어는 Word2Vec 이나 GloVe와 같은 기술들로 벡터로 변환될 수 있다.
- RNN 모델들은 이러한 시퀀스 벡터들을 입력으로 받아 처리하는 NLP의 주요 모델들이다.
- 어탠션과 트랜스포머의 등장으로 NLP는 비약적으로 성능이 향상되었다.
- 트랜스포머와 마찬가지로 고급 NLP들은 원래 기계 번역 작업을 위해 개발되었다.
- 초기에는 NLP 작업마다 따로 모델들이 설계되었다. 하지만 트랜스포머가 소개된 이후로는 특정 작업에 필요한 추가적인 레이블링을 할 필요가 없어졌다. ex) BERT, GPT-3
- 게다가 이러한 모델들은 특정 작업에 특화된 전용 모델보다 높은 성능을 보여주었다.
- 현재에는 이러한 모델들이 수많은 NLP 작업에서 필수적인 부분이 되었다. 하지만 이러한 모델은 많은 양의 데이터로 학습을 시켜야 하기 때문에 자원이 많이 든다.
- 따라서 오늘날 NLP 연구는 거대 자본이 이끌게 되었다.

# Bag-of-Words

## Bag-fo-Words

1. 유니크한 단어들로 이루어진 단어집을 형성한다.
    - Example sentences: “John really really loves this movie“, “Jane really likes this song”
    - Vocabulary: {“John“, “really“, “loves“, “this“, “movie“, “Jane“, “likes“, “song”}
2. 유니크 단어들을 원핫 벡터로 표현한다.
    - John: [1 0 0 0 0 0 0 0]
    - really: [0 1 0 0 0 0 0 0]
    - loves: [0 0 1 0 0 0 0 0]
    - this: [0 0 0 1 0 0 0 0]
    - movie: [0 0 0 0 1 0 0 0]
    - Jane: [0 0 0 0 0 1 0 0]
    - likes: [0 0 0 0 0 0 1 0]
    - song: [0 0 0 0 0 0 0 1]
    - 이 단어들의 짝은 모두 거리가 $\sqrt {2}$이다. 또한 코사인 유사도가 0이다.
3. 문장은 이 벡터들의 합으로 표현해줄 수 있다.
    - Sentence 1: “John really really loves this movie“
        - John + really + really + loves + this + movie: [1 2 1 1 1 0 0 0]
    - Sentence 2: “Jane really likes this song”
        - Jane + really + likes + this + song: [0 1 0 1 0 1 1 1]

## NaiveBayes Classifier

- 베이즈 법칙을 통해 문서를 분류하기

![Untitled.png](/images/2021-02-16/030/Untitled.png)
- 클래스중 가장 $P(c\mid d)$를 크게하는 클래스가 해당 문서의 클래스일 것이다.
- $P(d\mid c)P(c) = P(w_1,w_2,...,w_n\mid c) \rightarrow P(c)\prod_{w_i \in W} P(w_i \mid c)$
- 조건부 독립이라는 가정하에 위의 식이 성립한다.

![Untitled%201.png](/images/2021-02-16/030/Untitled%201.png)
![Untitled%202.png](/images/2021-02-16/030/Untitled%202.png)
- $P(c_{NLP}\mid d_5)$가 더 크므로 문서는 NLP 클래스로 분류된다는 사실을 알 수 있다.

# Word Embedding

- 단어를 벡터로 표현하는 것이다.
- 예를 들어 'cat' 와 'kitty'는 비슷한 단어이므로, 그들은 비슷한 벡터 형태를 가진다. 즉 두 벡터의 거리는 짧을 것이다.
- 하지만 'hamburger'은 위의 두 단어와 비슷하지 않다. 따라서 그들은 다른 벡터 형태를 가질 것이다. 즉 두 벡터의 거리는 멀 것이다.
- 한 단어의 뜻은 그 단어와 함께 나오는 단어들의 분포로 근사할 수 있다.

## Word2Vec

![Untitled%203.png](/images/2021-02-16/030/Untitled%203.png)
- 슬라이딩 윈도우를 이용해 같이 나오는 단어들을 각각 짝으로 맞춘다. 예를 들어 크기 3짜리 윈도우라면 'study'의 경우, 앞 뒤로 ('study', 'I'), ('study', 'math') 두 개의 짝이 나올 것이다.
- 윈도우를 통해 나온 짝을 2층 신경망의 입출력으로 학습시킨다.
- 이 때 히든 레이어의 노드 개수는 하이퍼 파라미터이다.
- $W_1$의 column의 수와 $W_2$의 row의 수는 총 단어의 개수이다. 각각의 column과 row는 각 단어의 벡터를 의미한다.
- 일반적으로 단어 벡터는 $W_1$을 사용하게 된다.

## GloVe: Global Vectors for word Representation

- 동시에 나오는 단어들을 미리 계산해서 두번 이상 나오는 짝에 대해서 여러번 학습시킬 필요가 없다.
- 학습이 빠르고 작은 말뭉치에서도 잘 동작한다.
- 다음의 로스함수를 이용해 학습시킨다. 이 때, $P_{ij}$는 한 윈도우 내에서 두 단어가 동시 발생한 횟수이다.

![Untitled%204.png](/images/2021-02-16/030/Untitled%204.png)
