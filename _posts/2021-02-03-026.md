---
title: 12일차 - 최적화와 CNN
tags: [boostcamp]
published: true
use_math: true
comments: true
---
# Optimization

## Gradient Descent

- First-order iterative optimization algorithm for finding a local minimum of a differentiable function.

## Important Concepts in Optimization

### Generalization

- How well the learned model will behave on unseen data.
- 학습 데이터 사이와 테스트 데이터 사이의 차이

### Underfitting vs. Overfitting

![Untitled.png](/images/2021-02-03/26/Untitled.png)
### Cross-validation

- Cross-validation is a model validation technique for assessing how the model will generalize to an independent (test) data set.
- 로스 함수의 선택이나 레이어의 갯수 같은 하이퍼 파라미터를 비교할 때 사용한다.
- 테스트 데이타는 어떤 식으로든 학습에 사용해서는 안된다.

### Bias and Variance

- 비슷한 입력에 비슷한 출력을 가지는 것을 Variance가 낮다고 한다.
- 출력의 mean이 타켓보다 많이 클경우 bias가 크다고 한다.
- We can derive that what we are minimizing(**cost**) can be decomposed into the three different parts: bias$^2$, variance, and noise.

![Untitled%201.png](/images/2021-02-03/26/Untitled%201.png)
- 결과적으로 bias와 variance를 모두 낮출 수는 없으며 타협을 봐야 한다.

### Bootstrapping

- Bootstrapping is any test or metric that uses random sampling with replacement.
- 전체 데이터에서 임의의 샘플을 학습시킨 여러 모델을 가지고 일관성을 테스트하는 것

### Bagging vs. Boosting

- **Bagging**(**B**oostrapping **agg**regat**ing**)
- Multiple models are being trained with bootstrapping.
- ex) Base classifiers are fitted on random subset where individual predictions are aggregated (votiong or averaging).
- Boosting
- It focuses on those specific training samples that are hard to classify.
- a strong model is built by combining weak learners in sequence where each learner learns from the mistakes of the previous weak learner.
- Boosting은 여러 개의 모델을 만들어 하나의 강한 모델을 만드는 방식이다.

![Untitled%202.png](/images/2021-02-03/26/Untitled%202.png)
# Gradient Descent Methods

- Stochastic Gradient Descent : update with the gradient computed from a single sample.
- Mini-batch Gradient Descent : update with the gradient computed from a subset of data.
- Batch Gradient Descent : update with the gradient computed from the whole data.

### Batch-size Matters

![Untitled%203.png](/images/2021-02-03/26/Untitled%203.png)
- batch 사이즈가 커지면 일반적으로 sharp minimum으로 모이는 경향이 있다.
- 반면에 batch 사이즈가 작으면 일관적으로 flat minimum으로 모이게 된다.
- 그래프에서 보다시피 오차가 생길 때 flat minimum이 오차가 더 적은 것을 알 수 있다.

## Gradient Descent Methods

### Gradient Descent

![Untitled%204.png](/images/2021-02-03/26/Untitled%204.png)
### Momentum

![Untitled%205.png](/images/2021-02-03/26/Untitled%205.png)
### Nesterov Accelerated Gradient

![Untitled%206.png](/images/2021-02-03/26/Untitled%206.png)
### Adagrad

- **Adagrad** adapts the learning rate, performing larger updates for infrequent and smaller updates for frequent parameters.

![Untitled%207.png](/images/2021-02-03/26/Untitled%207.png)
- 학습이 되면 될 수록 Learning rate가 줄어들어 학습이 되지 않게 된다.

### Adadelta

- **Adadelta** extends Adagrad to reduce its monotonically decreasing the learning rate by restricting the accumulation window.

![Untitled%208.png](/images/2021-02-03/26/Untitled%208.png)
- $0 \leq \gamma \leq 1$
- [ ]  EMA = Exponential Moving  Average

### RMSprop

- **RMSprop** is an unpublished, adaptive learning rate method proposed by Geoff Hinton in his lecture.

![Untitled%209.png](/images/2021-02-03/26/Untitled%209.png)
### Adam

- Adaptive Moment Estimation(**Adam**) leverages both past gradients and squared gradients.

![Untitled%2010.png](/images/2021-02-03/26/Untitled%2010.png)
## Regularization

### Early Stopping

- 학습을 시키다가 loss가 커지면 그 때 학습을 종료 시키는 것
- Early Stopping를 위한 추가적인 validation data가 필요하다.

### Parameter Norm Penalty

- It adds smoothness to the function space.

![Untitled%2011.png](/images/2021-02-03/26/Untitled%2011.png)
- Weight가 작으면 함수가 smoother 해지고 그랬을 경우에는 Generalization 성능이 높다는 가정하에 이 방법을 사용한다.

### Data Augmentation

- More data are always welcomed.
- However, in most cases, training data are given in advance.
- In such cases we need data augmentation.

![Untitled%2012.png](/images/2021-02-03/26/Untitled%2012.png)
### Noise Robustness

- add random noises inputs or weights.

![Untitled%2013.png](/images/2021-02-03/26/Untitled%2013.png)
### Label Smoothing

- **Mix-up** constructs augmented training exampes by mixing both input and output of two randomly selected training data.
- **CutMix** constructs augmented training exaples by mixing inputs with cut and paste and outputs with soft labels of two randomly selected training data.

![Untitled%2014.png](/images/2021-02-03/26/Untitled%2014.png)
### Dropout

- In each foward pss, randomly set some neurons to zero.
- 이는 모델을 robust 하게 만드는 효과가 있다.

### Batch Normalization

- Batch normalization compute the empirical mean and variance independently for each dimension (layers) and normalize.
- There are different variances of normalizations.

![Untitled%2015.png](/images/2021-02-03/26/Untitled%2015.png)
# Convolution Neural Network

- 기존의 MLP의 경우에는 fully connected 구조이기 때문에 가중치가 몹시 커질 수 밖에 없는 구조였다.
- Convolution 연산은 이와 달리 커널(kernel)을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조이다.

![Untitled%2016.png](/images/2021-02-03/26/Untitled%2016.png)
- 모든 i에 대해 적용되는 커널은 V로 같고 커널의 사이즈만큼 x 상에서 이동하면서 적용한다.
- Convolution 연산의 수학적 의미는 신호를 커널을 이용해 국소적으로 증폭 또는 감소시켜 정보를 추출 또는 필터링 하는 것이다.

![Untitled%2017.png](/images/2021-02-03/26/Untitled%2017.png)
![Untitled%2018.png](/images/2021-02-03/26/Untitled%2018.png)
- CNN에서 사용되는 연산은 엄밀히 말하면 뺄셈이 아닌 덧셈이 사용되므로 cross-correlation이지만, 역사적으로 계속 Convolution이라 불렀으니 Counvolution이라 한다.
- 커널은 정의역 내에서 움직여도 변하지 않고(translation invariant) 주어진 신호에 국소적(local)으로 적용한다.

## 다양한 차원에서의 Convolution

![Untitled%2019.png](/images/2021-02-03/26/Untitled%2019.png)
![Untitled%2020.png](/images/2021-02-03/26/Untitled%2020.png)
![Untitled%2021.png](/images/2021-02-03/26/Untitled%2021.png)
- 3차원 Convolution의 경우에는 각 채널마다 커널이 존재한다. 그래서 각 채널에 해당하는 커널을 입력에 convolution을 해준 값을 전부 더하면 된다.

![Untitled%2022.png](/images/2021-02-03/26/Untitled%2022.png)
![Untitled%2023.png](/images/2021-02-03/26/Untitled%2023.png)
- 출력을 3차원으로 만드려면 커널을 더 늘려주면 된다.

## Backpropagation of Convolution

![Untitled%2024.png](/images/2021-02-03/26/Untitled%2024.png)
