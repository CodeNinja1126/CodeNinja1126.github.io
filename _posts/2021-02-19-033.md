---
title: 19일차 - NLP Sequence to Sequence with Attention
tags: [boostcamp]
published: true
use_math: true
comments: true
---
# 19일차 - Transformer

# Transformer

![Untitled.png](/images/2021-02-19/033/Untitled.png)
- 각 입력 벡터($x$)를 attention처럼 사용한다. 스스로가 attention이 되므로 self-attention이라는 이름이 붙었다.
- 하지만 자기 자신을 어텐션으로 사용해 $h$벡터를 구성하게 된다면 그 벡터에는 자기 자신에 대한 정보만 가중되어서 나타날 것이다.
- 그래서 transformer 방식은 이를 방지하기 위한 방법을 구현하였다.
- 일단 자기 자신을 $W^Q$를 통해 attention처럼 동작하는 Query벡터로 변환해 준다.
- 문장의 모든 입력값을 $W^K$를 통해 Key벡터로 변환한다.
- Query벡터를 각 Key벡터와 내적하고 소프트맥스 함수를 통해 가중치를 계산한다.
- 한편 문장의 모든 입력값을 $W^V$를 통해 Value벡터로 변환한다.
- 각 Value벡터를 위에서 Key벡터와 Query벡터로 구했던 가중치에 각각 곱하고 더해 디코더의 히든 스테이트 벡터를 만든다.
- 그림을 보면 벡터간 가중치가 자기 자신인 'I'보다 'home'이 0.7로 더 큰 것을 알 수 있다.
- 정리하면 Query 벡터와 Key벡터를 통해 입력 간의 가중치를 계산하고, Value 벡터를 통해 각 입력에 해당하는 값을 출력한다.
- 트랜스포머는 기존의 RNN의 고질적인 문제인 Rong-term dependency를 해결한 모델로 시퀀스가 아무리 길더라도 Query를 통해 가중치가 높게 나오면 먼 거리의 정보도 가져올 수 있다.

![Untitled%201.png](/images/2021-02-19/033/Untitled%201.png)
- 수식으로 표현하면 이와 같으며, $q, k$벡터는 같은 차원을 가져야 하지만 $v$벡터는 그렇지 않아도 된다.
- 이를 행렬 연산으로 나타내면 다음과 같다.

![Untitled%202.png](/images/2021-02-19/033/Untitled%202.png)
- 실제 연산에는 $Q,K,V$모두 같은 shape을 가지게 된다.

![Untitled%203.png](/images/2021-02-19/033/Untitled%203.png)
- 내적 기반으로 유사도를 계산하면 두 벡터의 각 값들을 확률 변수로 보았을 때, 계산 후 값의 분산이 두 벡터의 차원만큼 커지는 현상이 생길 수 있다.
- 이러면 소프트맥스 후 몇몇 값이 엄청 커지는 경우가 생길수 있다. 그리고 그래디언트 배니싱이 일어나고 학습이 안 될 수 있다.
- 그래서 소프트맥스 함수를 적용하기 전에 $\sqrt {d_k}$를 element-wise하게 나누어주는 것이다.

# Multi-Head Attention

![Untitled%204.png](/images/2021-02-19/033/Untitled%204.png)
- 기존의 single attention의 문제는 각 단어에 대해서 다른 단어들과 상호작용할 수 있는 방법이 단 한가지였다는 것이다.
- 그래서 이런 문제점을 탈피하고자 여러 개의 W행렬을 통해 여러 개의 헤드를 만들어 단어들과 상호작용하는 다양한 방법을 만드는 것이다.

![Untitled%205.png](/images/2021-02-19/033/Untitled%205.png)
![Untitled%206.png](/images/2021-02-19/033/Untitled%206.png)
- Multi-Head attention을 도식화 하면 이렇다.
- 하지만 일반적인 구현은 이렇지 않다. 이렇게 하면 패러미터가 너무 많아지고 계산해야 하는 값도 너무 많으므로 이렇게 구현하지 않는다.
- 일반적으로는 q,k,v 벡터를 헤드의 개수 n개로 나누어서 n개의 어탠션 벡터를 만든다. 이렇게 할 경우 데이터나 패러미터의 양은 그대로 이면서 각 입력값에 대해 다양한 어탠션을 적용할 수 있게 된다.

## Complexity of Multi-Head Attention

![Untitled%207.png](/images/2021-02-19/033/Untitled%207.png)
- self-attention의 경우에는 메모리 복잡도는 문장의 길이의 따라 엄청나게 높아질 수 있지만 코어가 무한하다는 가정하에 한번의 시퀀스에 값을 계산할 수 있으며 지나야 하는 통로도 1개이다.
- 반면 바닐라 RNN은 메모리복잡도가 문장의 길이의 엄청난 영향을 받지는 않지만 매 시퀀스를 모두 지나서 히든스테이트를 계산하므로 n번의 시퀀스를 통해서 값을 받을 수 있다. 또한 지나야 하는 통로도 매 시퀀스를 통과해야하므로 최대 n개의 통로를 지나야 할 수 있다.

## Layer Normalization

![Untitled%208.png](/images/2021-02-19/033/Untitled%208.png)
- Layer Normalization을 해주게 되면 그래디언트 배니싱 문제를 해결할 수 있다.
- 이 때 x벡터와 더해주어야 하므로 Multi-Head attention의 아웃풋의 차원은 x벡터와 같아야 한다.
- Layer Norm의 역할은 분산을 1로 평균을 0으로 만들기 위함이다.
- 이 후 Feed Forward를 통과후 한번더 Layer Norm을 통해 값을 안정화 시킨다.

![Untitled%209.png](/images/2021-02-19/033/Untitled%209.png)
- 예컨데 위와 같은 경우는 $(3,5,-2)$벡터를 정규화를 통해 분산과 평균을 1과 0으로 조절한 것이다.
- 이 때 아래 함수에 값을 넣으면 평균은 3이 되고 분산은 $2^2$이 곱해진다.

## Positioning Encoding

![Untitled%2010.png](/images/2021-02-19/033/Untitled%2010.png)
- 트랜스포머에서는 순서에 대한 연산이 없다. 이를 위해 입력 벡터에 순서에 대한 값을 더해 주게 된다. 이 때 벡터의 차원 개만큼 삼각함수를 만들어 해당 벡터의 순서에 해당하는 값를 더해주게 된다.

## Warm-up Learning Rate Scheduler

![Untitled%2011.png](/images/2021-02-19/033/Untitled%2011.png)
- 러닝 레이트를 점점 올리다가 학습이 진행됨에 따라 동적으로 낮추어주는 방법이다. 이 방법은 경험적으로 더 좋은 결과를 낸다고 알려져 있다.

## Decoder

![Untitled%2012.png](/images/2021-02-19/033/Untitled%2012.png)
- 인코더 연산이 수행된 후 디코더에 처음 <SOS> 토큰을 입력한다. 첫 레이어는 Masked Multi-head layer이다. 이 layer는 아래에서 설명한다. 이 layer에서 출력된 값을 다음 Multi-head layer에 Query 벡터로 사용하게 된다.

![Untitled%2013.png](/images/2021-02-19/033/Untitled%2013.png)
- 두번째 multi-head layer에서는 인코더에서 계산한 값을 가져와  V와 K벡터로 사용하게 된다. 그 후 Feed Foward 해준다.

![Untitled%2014.png](/images/2021-02-19/033/Untitled%2014.png)
- 마지막으로 타겟 vocabulary 사이즈에 맞게 변환해주기 위해 Linear 변환을 하고 이를 소프트 맥스해 적당한 단어를 출력하게 된다.

## Masked Self-Attention

![Untitled%2015.png](/images/2021-02-19/033/Untitled%2015.png)
- 디코더에서는 이전의 단어는 존재하지만 이후 만들 단어는 존재하지 않는다. 그렇기 때문에 생성되지 않은 값에 출력을 막기 위해서 후처리적으로 생성되지 않은 단어에 대한 소프트맥스 값($softmax(Q\odot K^T)$)을 0으로 마스킹해준다. 이후 벡터의 총합을 1로 만들어준다.
- 소프트맥스 하기 전 마스킹된 값을 $-\infin$로 설정해 주어도 결과는 위와 같다.
