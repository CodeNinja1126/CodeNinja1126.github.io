---
title: 17일차 - RNN 기본
tags: [boostcamp]
published: true
use_math: true
comments: true
---
# 17일차 - RNN 기본

# Basics of Recurrent Neural Networks (RNNS)

![Untitled.png](/images/2021-02-17/031/Untitled.png)
- RNN의 기본 구조는 현재 $t$에서 이전의 값 $h_t$를 사용한다는 것이 가장 큰 특징이다.
- 이를 통해 RNN은 시퀀스 데이터를 학습할 수 있게 된다.

## How to calculate the hidden state of RNNs

![Untitled%201.png](/images/2021-02-17/031/Untitled%201.png)
- 위와 같은 함수를 통해 한번의 스텝을 계산한다.
- 이 때 매 스텝마다 함수와 함수의 파라미터들은 바뀌지 않는다는 것을 명심하라.

![Untitled%202.png](/images/2021-02-17/031/Untitled%202.png)
- 함수는 위와 같다. $\tanh()$는 활성함수이다. 그리고 각 $W_{hh}$ 와 $W_{xh}$의 의미는 다음과 같다.

![Untitled%203.png](/images/2021-02-17/031/Untitled%203.png)
![Untitled%204.png](/images/2021-02-17/031/Untitled%204.png)
- $W_{xh}, W_{hh}$는 $x_t, h_{t-1}$를 $h_{t}$로 보내주는 역할을 한다.

## Types of RNNs

### One-to-one

![Untitled%205.png](/images/2021-02-17/031/Untitled%205.png)
- 일반적인 형태의 신경망을 말한다.

### One-to-many

![Untitled%206.png](/images/2021-02-17/031/Untitled%206.png)
- 입력은 한번만 하고 여러 개의 아웃풋을 얻는 경우, 이 때는 정말로 매번 입력이 없는 것이 아니고 0으로 채워진 벡터 혹은 매트릭스를 입력한다.

### Many-to-one

![Untitled%207.png](/images/2021-02-17/031/Untitled%207.png)
- 모든 입력이 끝난 후 마지막에 한번만 아웃풋을 하는 경우, 예로는 문장의 감정 상태를 파악하는 모델에 사용될 수 있다.

### Many-to-many

![Untitled%208.png](/images/2021-02-17/031/Untitled%208.png)
- 모든 시퀀스 데이터를 입력하고 난 후 다시 시퀀스 형태로 데이터를 받는 경우, 이 때 데이터는 입력이 끝난 후 출력된다. 예로는 기계 번역이 있다.

![Untitled%209.png](/images/2021-02-17/031/Untitled%209.png)
- 시퀀스 데이터를 받아 리얼 타임으로 아웃풋을 받아야 할 때 사용한다. 대표적으로는 프레임 레벨의 영상 분류가 있다.

## Character-level Language Model

- 'hello'라는 단어를 예를 들어보자.

![Untitled%2010.png](/images/2021-02-17/031/Untitled%2010.png)
- 이 때 소프트 맥스 함수를 통해 $W_{hy}$를 학습시키게 될 것이다.
- 예측값을 NLP의 입력으로 사용해 첫 한개의 입력으로 시퀀스 데이터를 예측할 수 있다.
- 캐릭터레벨로 학습시킬 경우 공백, 줄바꿈문자, 맞춤표까지 의미가 크므로 모두 학습에 사용한다.
- 이렇게 논문, 문학, 코드 등을 학습시키면, 그 특징을 그럴듯하게 학습해낸다.
- 이 때 처음부터 첫 글자만을 입력으로 사용해서 학습시키게 되면 학습속도가 매우 더디므로, Teach Forcing이라는 기법을 사용해 정답레이블을 인풋에 넣어주게 된다.

## Backpropagation through time (BPTT)

![Untitled%2011.png](/images/2021-02-17/031/Untitled%2011.png)
- RNN의 역전파는 현재 아웃풋의 로스를 계산하기 위해서는 이전 시퀀스를 모두 계산해야 하므로 매우 많은 연산 자원이 필요하다.

![Untitled%2012.png](/images/2021-02-17/031/Untitled%2012.png)
- 그래서 일정 크기로 전체 시퀀스를 잘라 그 안에서만 역전파와 순전파를 진행한다.

![Untitled%2013.png](/images/2021-02-17/031/Untitled%2013.png)
![Untitled%2014.png](/images/2021-02-17/031/Untitled%2014.png)
- $h$벡터의 특정 위치의 값이 음수와 양수 상태일 때를 시각화한 것으로 특정 셀이 이전의 상태를 보존하고 있음을  알 수 있다.

## Vanishing/Exploding Gradient Problem in RNN

![Untitled%2015.png](/images/2021-02-17/031/Untitled%2015.png)
- 바닐라 RNN의 경우에는 계속해서 $W_{hh}$를 곱해주므로 그래디언트가 너무 커지거나 작아져 버리는 문제점이 존재한다.

# Long Short-Term Memory(LSTM)

- 어떠한 변환없이 쭉 전달되는 셀 스테이트를 넘긴다.
- 이를 통해 긴 스텝 이후의 의존도 문제를 해결한다.

![Untitled%2016.png](/images/2021-02-17/031/Untitled%2016.png)
![Untitled%2017.png](/images/2021-02-17/031/Untitled%2017.png)
- 이때 $\tanh$는 정보의 변환을, $\sigma$는 정보의 비율을 결정하기 위한 함수라고 생각하면 된다.

### Gate gate

![Untitled%2018.png](/images/2021-02-17/031/Untitled%2018.png)
- 이전까지의 정보를 담고 있는 $C_{t-1}$을 얼마나 기억할 것인 지를 결정하는 게이트

### Gate gate

![Untitled%2019.png](/images/2021-02-17/031/Untitled%2019.png)
- 현재의 정보를 얼마나 담을 것 인지를 결정하는 게이트

### Output gate

![Untitled%2020.png](/images/2021-02-17/031/Untitled%2020.png)
- $h_t$의 값을 결정하는 게이트, 이 때 $h_t$는 미래의 시퀀스에 장기적인 영향력을 주는 것이 아니라 다음 시퀀스의 예측에 직접적으로 영향을 주는 값이다.

# Gated Recurrent Unit(GRU)

![Untitled%2021.png](/images/2021-02-17/031/Untitled%2021.png)
- 인풋 게이트인 $z_t$를 이용해  현재의 정보인 $\tilde{h_t}$ 와 과거의 정보인 $h_t$의 조합 비율을 결정한다.
- LSTM과 GRU는 이전의 정보를 전달하는데 곱셈을 사용하지 않고 덧셈을 사용하면서 그래디언트를 과거로부터 잘 전달할 수 있게 되었다.
