---
title: 15일차 - Generative Models
tags: [boostcamp]
published: true
use_math: true
comments: true
---
# 15일차 - Generative Models

# Learning a Generative Models

- Generative Model은 단순히 생성을 하는 모델만을 말하는 것은 아니다.
- Generative Model은 생성과 함께 생성된 결과에 대해 구별하는 기능을 포함한다.
- 어떠한 결과물에 대해서 $p(x)$를 얻어낼 수 있는 모델을 explicit model이라 한다. 그리고 그렇지 않은 모델을 implicit 모델이라고 한다.
- $p(x)$가 커지면 의도한 결과물에 가까울 것이고 작다면 그렇지 않을 것이다. (anomaly detection, 이상행동 감지)
- Unsupervised representation learning 에서 사용되는 feature learning도 Generative model이라 보는 시각도 존재한다.

## Basic Discrete Distribution

![Untitled.png](/images/2021-02-14/Untitled.png)
### Example

- n개의 픽셀을 가진 바이너리 이미지를 생각해보자
- 이때 나올수 있는 상태 가지수는 $2^n$이다.
- 이 때 우리가 각 상태를 특정할 수 있는 parameter는 총 $2^n - 1$이다.
- 반면 우리가 이 때 각 픽셀에 대해 독립적인 사건으로 생각해 보자.
- 이 때 나올 수 있는 상태 가지수는 $2^n$으로 같다.
- 하지만 이 때의 parameter은 총 n이다.

### Conditional indipendence

![Untitled%201.png](/images/2021-02-14/Untitled%201.png)
- 위의 체인 룰을 가지고 다음과 같이 식을 만들 수 있다.

![Untitled%202.png](/images/2021-02-14/Untitled%202.png)
- 이 경우 여전히 Parameter은 여전히 $2^n - 1$이다.

![Untitled%203.png](/images/2021-02-14/Untitled%203.png)
- 하지만 이렇게 Markov 가정을 이용하면 parameter를 줄일 수 있게 된다.
- 이렇게 parameter를 대폭 줄이면서도 어느 정도 입력 간에 의존적인 상태를 유지할 수 있게 된다.

## Auto-regressive model

- Conditional indipendence을 활용하는 모델이다.
- 우리는 $p(x)$를 학습시키는 것이 목적이다.
- 이 값을 체인룰을 통해 식을 길게 전개한다.
- 우리는 임의의 값에 대해 순서를 정해야 한다. 예를 들어 이미지의 경우 가로 방향 세로 방향 대각선 방향 등으로 순서를 정할 수 있다.

### NADE : Neural Autoregressive Density Estimator

![Untitled%204.png](/images/2021-02-14/Untitled%204.png)
- NADE는 주어진 입력에 대해 확률을 계산할 수 있는 explicit 모델이다.
- 주어진 이미지에 대해서 각 픽셀의 확률을 계산하는 것은 체인룰로 $p(x)$를 전개하고 모든 값을 곱하는 것이다.
- 연속 확률 변수를 모델링하는 경우, 가우시안 혼합 모델을 사용할 수 있다. 
- 참조 [https://untitledtblog.tistory.com/133](https://untitledtblog.tistory.com/133)

## pixel RNN

- auto-regressive model을 정의하기 위해 RNN을 사용할 수도 있다.

![Untitled%205.png](/images/2021-02-14/Untitled%205.png)
- 체인의 순서에 기반한 pixel RNN에는 다음과 같은 두가지 모델이 있다.

![Untitled%206.png](/images/2021-02-14/Untitled%206.png)
# Latent Variable Models

- Is an **autoencoder** a generative model?
- Latent Variable Model은 implicit 모델이다.

### Variational Auto-encoder

- **Variational inference(VI)**
- VI의 목적은 **posterior distribution**을 가장 잘 매칭시키는 **variational distribution**을 최적화하는 것이다.
- **Posterior distribution**: $p_\theta(z\mid x)$
- **Variational distribution**: $q_\phi(z \mid x)$
- 특히 우리는 KL divergence를 사용하여 실제 Posterior와의 거리를 최소화하는 Variantional distribution를 찾고 싶다.

![Untitled%207.png](/images/2021-02-14/Untitled%207.png)
- 하지만 우리는 Posterior가 무엇인지도 모르는데 어떻게 Variantional distribution을 찾을 것인가

![Untitled%208.png](/images/2021-02-14/Untitled%208.png)
- ELBO(evidence lower bound)를 이용할 수 있다. 
- 참조 [https://hugrypiggykim.com/2018/09/07/variational-autoencoder와-elboevidence-lower-bound/](https://hugrypiggykim.com/2018/09/07/variational-autoencoder%EC%99%80-elboevidence-lower-bound/)

![Untitled%209.png](/images/2021-02-14/Untitled%209.png)
### Variational Auto-encoder의 주요 한계

![Untitled%2010.png](/images/2021-02-14/Untitled%2010.png)
### Adversarial Auto-encoder

![Untitled%2011.png](/images/2021-02-14/Untitled%2011.png)
# GAN(Generative Adversarial Network)

![Untitled%2012.png](/images/2021-02-14/Untitled%2012.png)
![Untitled%2013.png](/images/2021-02-14/Untitled%2013.png)
- GAN은 생성과 분류를 번갈아 가며 학습한다. 생성을 하면 그것을 진짜 이미지와 섞어 분류를 학습시키고, 다시 그것을 기반으로 생성을 학습시킨다. 이를 계속 반복한다.

![Untitled%2014.png](/images/2021-02-14/Untitled%2014.png)
![Untitled%2015.png](/images/2021-02-14/Untitled%2015.png)
- 결과적으로 GAN은 discrimainator와 generator 사이의 JSD를 최소화시키는 모델이라 할 수 있다
- GAN역시 implicit 모델이다.
