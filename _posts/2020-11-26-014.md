---
title: "Pandas의 기본적인 사용법"
tags: [python]
published: false
---
이번 포스팅은 edwith 머신러닝을 위한 파이썬 강의에서 배운 

pandas의 기본적인 사용법에 대해 이야기한다.

- 구조화된 데이터의 처리를 지원하는 Python 라이브러리
- 고성능 Array 계산 라이브러리인 Numpy와 통합하여, 강력한 "스프레드시트" 처리 기능을 제공
- 인덱싱, 연산용 함수, 전처리 함수 등을 제공함

다음과 같이 선언해 많이 사용함

```python
import pandas as pd
```

# 데이터 불러오기

```python
data_url = '[데이터 url]'
df_data = pd.read_csv(data_url, sep'\s+', header = None)
# csv 타입 데이터 로드, seperate는 빈 공간으로 지정함

df_data.head()
# 첫 5개의 데이터를 가져옴
```

데이터들은 numpy의 array형태로 이루어져 있음. 이상적인 경우에는 csv 형태로 데이터를 받아올 수도 있지만 대부분의 경우에는 데이터베이스에서 자료를 추출해와 처리하기 쉬운 형태로 바꾸어주어야 한다.

# Series

```python
list_data = [1,2,3,4,5]
example_obj = pd.Series(data = list_data)
'''
0 1
1 2
2 3
3 4
4 5
dtype: int64
'''
```

시리즈는 numpy ndarray의 자녀 클래스이다. 인덱스와 index와 values의 형태로 이루어져 있다.

```python
list_data = [1,2,3,4,5]
list_index = ['a', 'b', 'c', 'd', 'e']
example_obj = pd.Series(data = list_data, index = list_index)
'''
a    1
b    2
c    3
d    4
e    5
dtype: int64
'''
```

이렇게 인덱스를 설정해 줄 수도 있다.

```python
dict_data = {'a':1, 'b':2, 'c':3, 'd':4, 'd':5}
example_obj = pd.Series(dict_data, dtype = np.float, name='example_data')
'''
a    1.0
b    2.0
c    3.0
d    5.0
Name: example_data, dtype: float64
'''
```

dict 타입으로 데이터를 넣어주면 인덱스 리스트를 따로 만들어줄 필요가 없다.

위는 추가적으로 데이터 타입과 이름을 설정해줄 수 있음을 보였다.

```python
value = example_obj['a']
example_obj['b'] = 7.0
```

이와 같이, 일반 dict처럼 값을 참조 및 저장할 수 있다.

```python
example_obj[['a','b']]
```

이렇게 선언하면 해당 인덱스와 데이터를 포함하는 새로운 시리즈를 생성할 수 있다.

```python
example_obj[example_obj > 3.0]
```

boolean index도 사용 가능하다.

## Operation

```python
s1 + s2
```

 시리즈간 덧셈이 가능하며, 같은 index 값을 가진 값끼리 덧셈이 된다. 만약 겹치는 index가 존재하지 않으면, NaN으로 값이 반환된다. 사칙연산이 모두 가능하다.

# DataFrame

```python
dict = {'first_name': ['hyun', 'ga', 'woo'],\
        'last_name': ['han', 'kim', 'han'],\
        'age': [28, 30, 24]}
df = pd.DataFrame(dict, columns = ['first_name', 'last_name', 'age'])
'''
  first_name	last_name	 age
0	hyun	      han	       28
1	ga	        kim	       30
2	woo	        han	       24
'''
```

 이렇게 dict 타입으로 DataFrame을 선언해 줄 수도 있지만 일반적으로는 csv 형태로 파일을 가져오기 때문에 이렇게 사용할 일은 거의 없다. 

## columns

columns는 내가 보고 싶은 columns 값만 포함해 DataFrame을 짤 수 있다.

```python
df = pd.DataFrame(df, columns = ['first_name', 'last_name'])
'''
  first_name	last_name
0	hyun	      han
1	ga	        kim
2	woo	        han
'''
```

위에서 만들었던 DataFrame을 이용해 새로운 DataFrame을 만든 것이다.

다음과 같이 새로운 column을 추가해 줄 수도 있다.

```python
df = pd.DataFrame(df, columns = ['first_name', 'last_name', 'sex'])
'''
  first_name	last_name	sex
0	hyun	      han	      NaN
1	ga	        kim	      NaN
2	woo	        han	      NaN
'''
```

이 때 값은 모두 NaN임을 주의해야 한다.

```python
df['first_name']
'''
0    hyun
1      ga
2     woo
Name: first_name, dtype: object
'''
```

이렇게 Series의 형태로 값을 받을 수도 있다.

```python
df['old'] = df.age > 25
```

이렇게 사용하면 새롭게 column을 생성하고 값을 넣어줄 수도 있다.

```python
del df['old']
```

짐작하겠지만 이렇게 하면 column을 삭제할 수도 있다.

## location

```python
df.loc[1]
'''
first_name     ga
last_name     kim
age            30
Name: 1, dtype: object
'''
```

이렇게 선언하면 인덱스가 1인 데이터를 가져오게 된다.

```python
df.loc[[1,2],['first_name', 'last_name']]
```

이러한 형태로 원하는 column과 index number를 지정해 줄수도 있다.

pandas에는 iloc라는 기능도 있는데 이는 다음과 같다.

```python
df.iloc[1]
'''
first_name     ga
last_name     kim
age            30
Name: 1, dtype: object
'''
```

 선언했던 df의 경우에는 실제로 자료에 저장된 순서와 인덱스의 번호가 일치한다. 그래서 df.loc[1]과 같은 결과가 나오게 된 것이다. 하지만 자료의 순서와 인덱스의 번호가 일치 하지 않을 경우에는 다른 결과가 나올 수 있다. 즉, loc의 경우에는 인덱스 값을 기준으로하고, iloc는 저장된 자료의 순서를 기준으로 한다.

다음과 같이 사용하는 방법도 있다.

```python
df.iloc[:2, :2]
```

첫번째 값은 row, 두번째 값은 column이다.

## operation

```python
df1 + df2
```

 index 값과 column값이 모두 같은 값끼리 연산하게 된다. 이 역시 겹치지 않는 값이 있으면 NaN을 반환한다. 그럴 경우 다음과 같은 메소드를 사용한다.

```python
df1.add(df2.fill_value = 0)
```

 이렇게 되면 존재하지 않는 값을 0으로 생각하고 계산을 하게 된다. 예를 들어, df1의 값이 3인데 df2에 이에 대응하는 값이 없다면 그 값을 0으로 하고, 결과 값은 3으로 반환하게 된다. add, sub, div, mul 사칙 연산 메소드가 모두 존재한다. 그리고 연산 메소드를 수행해도 원본 데이터는 변경이 없다.

```python
df1 + s1
```

 DataFrame과 시리즈 간에도 덧셈이 가능하며, 브로드 캐스팅이 이루어진다. 이 때, 덧셈은 column을 기준으로 이루어지며 시리즈의 index 값과 DataFrame의 column가 일치하는 값에 대해 덧셈이 이루어진다. 이 때에도 서로 대응하는 값이 없으면 NaN을 반환한다.

```python
df1.add(s1, axis = 0)
```

 이렇게 축을 지정해주면 DataFrame의 index값에 대응 시켜서 덧셈을 할 수도 있다.

## 기타 기능들

```python
df.values() # ndarray의 형태로 반환해주는 기능
df.T #transpose 해서 반환해주는 기능
df.to_csv() # csv로 변환해서 반환
df.index 
'''
DataFrame의 index 값이다. 참조도 가능하고 데이터 프레임의 데이터 수와 같은 크기의 리스트나 series를 할당할 수도 있다.
df.index = df['account']
'''
```

# Selection & Drop

## Selection

```python
df['account'].head(3)
'''
'account' column의 첫 3개의 데이터를 시리즈의 형태로 가져옴
'''
df[['account','street', 'state']].head(3)
'''
지정해준 3개의 column을 가진 3개의 데이터를 DataFrame의 형태로 반환
'''
```

```python
df[:3]
```

index number를 기준으로 3까지의 데이터를 DataFrame의 형태로 반환, 이 때의 index number은 index의 값이 아닌 자료가 저장된 순서라는 것을 주의 할것. 그리고 이 때, index number는 반드시 범위로 지정해 주어야 한다. 하나의 데이터를 지정할 때에는 위의 loc를 참고할 것.

```python
df['account'][:3]
```

`account` column의 data를 시리즈의 형태로 index number 3까지의 데이터를 시리즈의 형탤로 반환, 이 때도 index number는 자료가 저장되는 순서이다.

```python
df[df['acount'] > 30]
```

이렇게 boolean index역시 가능하다.

## Drop

```python
df.drop(1)
df.drop([1,2,3,4])
```

 row 단위의 데이터를 제거하는 메소드, 하지만 실제로 DataFrame 객체 내에서 데이터의 삭제가 이루어지지는 않고 drop된 DataFrame 객체를 반환할 뿐이다. 그래서 실제로 DataFrame 객체 내에서 데이터를 삭제해 주고 싶다면 다음과 같이 선언해 주어야 한다.

```python
df.drop(1, inplace = True)
```

inplace를 True로 설정해주면 실제 객체 내에서 데이터가 삭제된다.

```python
df.drop('city', axis = 1)
```

city column을 삭제할 수도 있다. 이 때 axis를 설정해주는 것에 주의해야 한다.

# lambda, map, apply

## map

시리즈의 데이터들을 다른 데이터로 덮어씌우는 메소드

```python
z = {1 : 'A', 2: 'B', 3:'C'}
s1.map(z)
```

 이렇게 선언하면 z의 key값과 s1의 data가 일치하는 row에 대해 데이터들이 덮어씌워지게 된다. 이 때, z에 일치하는 key값이 없는 row에 대해선 NaN이 들어가게 된다. 

```python
df["sex_code"] =  df.sex.map({"male":0, "female":1})
```

DataFrame에서는 위와 같이 새롭게 column을 생성하는 형태로 많이 사용된다.

아래와 같이 replace 메소드를 사용해도 같은 결과를 얻을 수 있다.

```python
df.sex.replace({"male":0, "female":1})
```

하지만 replace 함수는 그냥 사용했을 경우, 실제로 객체 내에 데이터를 변경시키지 않으므로 실제로 데이터를 변경시키려면 다음과 같이 선언해 주면 된다.

```python
df.sex.replace(
    ["male", "female"], 
    [0,1], inplace=True)
```

이는 또한 dict 형태 외에도 변경시킬 값 리스트와, 변경 후 값 리스트를 이용할 수 있음을 보인다.

## apply

map은 시리즈 내에 하나의 데이터 단위로 무언가를 적용할 때 사용했다. dataframe의 apply는 시리즈 전체에 대해 적용할 때 사용하는 메소드이다.

```python
f = lambda x : x.max() - x.min()
df.apply(f)
'''
age             53
job    progamergay
dtype: object
'''
```

 이렇게 사용하면 각 column 단위로 함수를 적용하게 된다. 결과 값으로는 인덱스 값은 column, 값은 함수의 결과 값으로 하는 시리즈를 출력하게 된다. 

시리즈 값을 반환하는 함수를 변수로도 넣을 수 있다. 이 경우에는 새롭게 DataFrame을 반환하게 된다. 

```python
def f(x):
    ser = pd.Series([x.min(), x.max()], index = ['min', 'max'])
    return ser

df.apply(f)
'''
    age	job
min	16	gamer
max	20	pro
'''
```

## applymap

applymap 메소드의 경우에는 DataFrame에 모든 값에 대해 함수를 적용할 수 있다.

```python
f = lambda x : 0
df.apply(f)
'''
  age	job
1	0	  0
2	0	  0
3	0	  0
''' 
```

하지만 이는 시리즈의 apply함수로도 같은 결과를 얻을 수 있다.

```python
df.age.apply(f)
'''
1    0
2    0
3    0
Name: age, dtype: int64
'''
```

시리즈에서 apply메소드를 실행할 경우에는, 시리즈 단위로 함수를 실행했던 DataFrame에서와는 다르게, 각 값 단위로 함수를 실행하게 된다.

# pandas 내장 함수

```python
df.describe()
```

DataFrame의 데이터의 주요점을 요약해서 보여준다.

```python
df.age.unique()
```

시리즈의 값들을 중복되지 않게 나열한 ndarray를 반환한다.

# Groupby

- SQL groupby 명령어와 같다.
- split → apply → combine
- 과정을 거쳐 연산함

```python
df.groupby('Team')['Points'].sum()
```

여기서 Team은 묶음의 기준이 되는 컬럼이고 Points는 적용받는 컬럼이다. 

```python
h_index = df.groupby(['Team', 'Year'])['Points'].sum()
```

 이런식으로 한 개이상의 컬럼을 묶을 수도 있다. 이 결과물도 결과적으로는 데이터프레임이다.

```python
h_index.unstack()
```

이렇게 할 경우, 다음과 같이 matrix 형태로 전환해준다.

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d3def400-adcf-4421-b93b-f63119d1cedb/1.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d3def400-adcf-4421-b93b-f63119d1cedb/1.png)

```python
h_index.sum(level = 0)
h_index.sum(level = 1)
'''
Team
Devils    1536
Kings     2285
Riders    3049
Royals    1505
kings      812

Year
2014    3181
2015    3078
2016    1450
2017    1478

'''
```

각 그룹별로 연산을 수행할 수 있다.

```python
grouped = df.groupby('Team')

for name, group in grouped:
    print(name)
    print(group)
'''
Devils
     Team  Rank  Year  Points
2  Devils     2  2014     863
3  Devils     3  2015     673
Kings
    Team  Rank  Year  Points
4  Kings     3  2014     741
6  Kings     1  2016     756
7  Kings     1  2017     788
Riders
      Team  Rank  Year  Points
0   Riders     1  2014     876
1   Riders     2  2015     789
8   Riders     2  2016     694
11  Riders     2  2017     690
Royals
      Team  Rank  Year  Points
9   Royals     4  2014     701
10  Royals     1  2015     804
kings
    Team  Rank  Year  Points
5  kings     4  2015     812
'''
```

이렇게 split된 상태를 추출 가능하다.

```python
grouped.get_group('Devils')
'''
Devils
     Team  Rank  Year  Points
2  Devils     2  2014     863
3  Devils     3  2015     673
Kings
    Team  Rank  Year  Points
4  Kings     3  2014     741
6  Kings     1  2016     756
7  Kings     1  2017     788
Riders
      Team  Rank  Year  Points
0   Riders     1  2014     876
1   Riders     2  2015     789
8   Riders     2  2016     694
11  Riders     2  2017     690
Royals
      Team  Rank  Year  Points
9   Royals     4  2014     701
10  Royals     1  2015     804
kings
    Team  Rank  Year  Points
5  kings     4  2015     812
'''
```

특정 key값을 가진 그룹의 정보만 추출할 수도 있다. 추출된 group 정보에는 세 가지 유형의 apply가 가능하다.

- Aggregation: 요약된 통계정보를 추출해 줌
- Transformation: 해당정보를 변환해줌
- Filtration: 특정정보를 제거하여 보여줌

## Aggregation

```python
grouped.agg(sum)
'''
				Rank  Year  Points
Team
Devils     5  4029    1536
Kings      5  6047    2285
Riders     7  8062    3049
Royals     5  4029    1505
kings      4  2015     812
'''
```

특정 컬럼에 여러개의 한수를 apply할 수도 있다.

```python
import numpy as np

grouped['Points'].agg([np.sum, np.mean, np.std])
'''
				sum        mean         std
Team
Devils  1536  768.000000  134.350288
Kings   2285  761.666667   24.006943
Riders  3049  762.250000   88.567771
Royals  1505  752.500000   72.831998
kings    812  812.000000         NaN
'''
```

## Transformation

- Aggregation과 달리 key값 별로 요약된 정보가 아니다
- 개별 데이터의 변환을 지원한다.

```python
score = lambda x: (x)
grouped.transform(score)
'''
		Rank  Year  Points
0      1  2014     876
1      2  2015     789
2      2  2014     863
3      3  2015     673
4      3  2014     741
5      4  2015     812
6      1  2016     756
7      1  2017     788
8      2  2016     694
9      4  2014     701
10     1  2015     804
11     2  2017     690
'''
```

max나 min같이 Series 데이터에 적용되는 함수들은 key값을 기준으로 그룹화된 데이터들에 적용된다.

```python
score = lambda x: (x.max())
grouped.transform(score)
'''
    Rank  Year  Points
0      2  2017     876
1      2  2017     876
2      3  2015     863
3      3  2015     863
4      3  2017     788
5      4  2015     812
6      3  2017     788
7      3  2017     788
8      2  2017     876
9      4  2015     804
10     4  2015     804
11     2  2017     876
'''
```

## Filter

- 특정 조건으로 데이터를 검색할 때 사용
- filter안에는 boolean 조건이 존재해야 한다.

```python
df.groupby('Team').filter(lambda x: len(x) >= 3)
'''
			Team  Rank  Year  Points
0   Riders     1  2014     876
1   Riders     2  2015     789
4    Kings     3  2014     741
6    Kings     1  2016     756
7    Kings     1  2017     788
8   Riders     2  2016     694
11  Riders     2  2017     690
'''
```

# Pivot Table

- 엑셀에서 보던 그 것
- index 축은 groupby와 동일함
- 컬럼에서 추가로 labelling값을 추가하여, value에 numeric type 값을 aggregation하는 형태

```python
df_phone = pd.read_csv("https://www.shanelynn.ie/wp-content/uploads/2015/06/phone_data.csv")
df.phone['date'] = df_phone['date'].apply(dateutil.parser.parse, dayfirst = True)
df_phone.pivot_table(['duration'],\
                      index = [df_phone.month, df_phone.item],\
                      columns = df_phone.network, aggfunc = 'sum',\
                      fill_value = 0)
'''
duration
network        Meteor Tesco  Three Vodafone      data landline special voicemail world
month   item
2014-11 call     1521  4045  12458     4316     0.000     2906       0       301     0
        data        0     0      0        0   998.441        0       0         0     0
        sms        10     3     25       55     0.000        0       1         0     0
2014-12 call     2010  1819   6316     1302     0.000     1424       0       690     0
        data        0     0      0        0  1032.870        0       0         0     0
        sms        12     1     13       18     0.000        0       0         0     4
2015-01 call     2207  2904   6445     3626     0.000     1603       0       285     0
        data        0     0      0        0  1067.299        0       0         0     0
        sms        10     3     33       40     0.000        0       0         0     0
2015-02 call     1188  4087   6279     1864     0.000      730       0       268     0
        data        0     0      0        0  1067.299        0       0         0     0
        sms         1     2     11       23     0.000        0       2         0     0
2015-03 call      274   973   4966     3513     0.000    11770       0       231     0
        data        0     0      0        0   998.441        0       0         0     0
        sms         0     4      5       13     0.000        0       0         0     3
'''
```

## Crosstab

```python
df_movie = pd.read_csv(" ./movie_rating.csv")
pd.crosstab(index=df_movie.critic,\
            columns = df_movie.title,\
            values = df_movie.rating,\
            aggfunc = 'first').fillna(0)
```

이는 아래의 pivot table과 유사하다.

```python
df_movie.pivot_table(['rating'], index = df_movie.critic,\
                    columns=df_movie.title,\
                    aggfunc = 'sum', fill_value = 0)
```

# Merge & Concat

## Merge

- SQL에서 많이 사용하는 Merge와 같은 기능
- 두 개의 데이터를 하나로 합침

```python
pd.merge(df_a, df_b, on = 'subject_id')
#subject_id 기준으로 merge
```

```python
pd.merge(df_a, df_b, left_on = 'subject_id', right_on = 'subject_id')
# 두 데이타프레임의 컬럼이름이 다를 때
```

```python
pd.merge(df_a, df_b, on = 'subject_id', how = [join method])
'''
join method를 다르게 해주고 싶을 때,
각 'left', 'right', 'inner', 'outer'의 값을 주면 원하는 방식으로 join이 가능하다.
'''
```

```python
pd.merge(df_a, df_b, on = 'subject_id', right_index = True, left_index = True)
#두 데이터 프레임의 인덱스를 기준으로 merge하고 두 subject_id는 그대로 살리는 방법이다.
```

## Concat

- 데이터를 좌우상하로 붙이는 것

```python
df_new = pd.concat([df_a, df_b])
df_new.reset_index()
```

혹은 다음과 같이 선언해 줄 수도 있다.

```python
df_a.append(df_b)
```

좌,우로 concat하는 방법은 다음과 같다.

```python
df_new = pd.concat([df_a,df_b], axis=1)
df_new.reset_index()
```

# DB Persistence

- 데이타 로딩시 db connection 기능을 제공함

```python
import sqlite3

conn = sqlite3.connect([db 주소])
cur = conn.cursor()
cur.excute([쿼리 문])
results = cur.fetchall()

```

이렇게 db에서 원하는 데이터를 가져올 수 있다.

```python
df = pd.read_sql_query([쿼리 문], conn)
```

db에서 데이터프레임을 생성하는 방법

## XLS persistence

- Dataframe의 엑셀 추출 코드
- Xls엔진으로 openpyxls 또는 XlsxWrite 사용

```python
writer = pd.ExcelWriter([저장할 xlsx 이름 및 주소], engine = 'xlsxwriter')
df_routes.to_excel(writer, sheet_name = 'Sheet1')
```

## Pickle persistence

- 가장 일반적인 python 파일 persistence
- to_pickle, read_pickle 함수 사용

```python
df.to_pickle([저장할 pickle 이름 및 주소])
```

다음은 읽어오기

```python
df = pd.read_pickle([pickle 주소])
```
