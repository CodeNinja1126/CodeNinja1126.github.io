---
title: 7일차 - 경사하강법
tags: [boostcamp]
published: true
use_math: true
comments: true
---
# 경사하강법

## 미분

- 미분(differentiation)은 변수의 움직임에 따른 **함수값의 변화를 측정하기 위한 도구로 최적화에서 제일 많이 사용하는 기법**

```python
import sympy as sym
from sympy.abc import x

sym.diff(sym.poly(x**2 + 2*x + 3), x)
# Poly(2*x + 2, x, domain='ZZ')
```

![1.png](/images/2021-01-27/1.png)

- 접선의 기울기를 알면, 어느 방향으로 점을 움직이면 함수값이 증가하는 지 감소하는 지 알수 있다.

![2.png](/images/2021-01-27/2.png)

![3.png](/images/2021-01-27/3.png)

- **경사상승**/**경사하강** 방법은 극값에 도달하면 움직임을 멈춘다.

## 경사하강법: 알고리즘

```python
var = init
grad = gradient(var)
while(abs(grad) > eps):
    var = var - lr * gra
    grad = gradient(var)

# gradient: 미분을 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건
```

- 컴퓨터로는 미분이 정확하게 0이 되도록 계산하는 것은 불가능하다. 따라서 eps를 설정해 종료조건을 설정해 주는 것
- lr값을 통해 경사를 내려가는 속도를 제어할 수 있다. 이 값은 주의해서 다뤄야 한다.

## 변수가 벡터라면?

- 미분은 **변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구**이다.

![4.png](/images/2021-01-27/4.png)

```python
import sympy as sym
from sympy.abc import x, y

sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)
#2𝑥+2𝑦−sin(𝑥+2𝑦)
```

## 그레디언트 벡터

![5.png](/images/2021-01-27/5.png)

![6.png](/images/2021-01-27/6.png)
## 경사하강법: 알고리즘

```python
var = init
grad = gradient(var)
while(norm(grad) > eps):
    var = var - lr * grad
    grad = gradient(var)

# gradient: 그레디언트 벡터를 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건
```

# 경사하강법: 선형회귀 계수 구하기

![7.png](/images/2021-01-27/7.png)

![8.jpg](/images/2021-01-27/8.jpg)

## 경사하강법 알고리즘

- 목적식을 최소화하는 $\beta$을 최소화하는 경사하강법 알고리즘

- $\beta^{(t+1)}\leftarrow\beta^{(t)}-\lambda\nabla_\beta\|y - X\beta^{(t)}\|$

- $\beta^{(t+1)}\leftarrow\beta^{(t)}-\dfrac\lambda n\dfrac{X^T(y-X\beta^{(t)})}{\|y - X\beta^{(t)}\|}$

- L-2 노름의 제곱을 최소화하여도 그 목적은 같다고 할 수 있다.

![9.png](/images/2021-01-27/9.png)

```python
for t in range(T):
    error = y - x @ Beta
    grad = - transpose(X) @ error
    beta = beta - lr * grad

# @는 행렬곱
# 이 방식은 종료조건을 학습횟수로 지정한 방식이다.
```

- 이론적으로 경사하강법은 미분가능하고 볼록한 함수에 대해선 **적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장**된다.
- 특히 선형회귀의 경우 목적식 $\|y - X\beta\|_2$는 **희귀계수 $\beta$에 대해 볼록함수**이기 때문에 알고리즘을 충분히 돌리면 수렴이 보장됨
- 하지만 **비선형회귀** 문제의 경우 목적식이 볼록하지 않을 수 있으므로 **수렴이 항상 보장되지는 않는다.**

## 확률적 경사하강법

- **확률적 경사하강법(stochastic gradient descent)**는 모든 데이터를 사용해서 업데이트하는 대신 데이터 한개 또는 일부 활용하여 업데이트한다.
- 볼록이 아닌(non-convex) 목적식은 SGD를 통해 최적화할 수 있다.
- 데이터를 한 개만 사용하면 그냥 SGD, 일부만 사용하면 mini-batch SGD라고 한다.
- 이 역시 만능은 아니지만 딥러닝의 경우 SGD가 경사하강법보다 실증적으로 더 낫다고 한다.

![10.png](/images/2021-01-27/10.png)

![11.png](/images/2021-01-27/11.png)

- 배치를 사용할 때마다 곡선 모양이 바뀌므로 극소점을 탈출할 가능성이 생긴다.
