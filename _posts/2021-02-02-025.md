---
title: 11일차 - 베이즈 통계학과 Deap learning Basic
tags: [boostcamp]
published: true
use_math: true
comments: true
---

# 베이즈 통계학

- 조건부 확률 $P(A|B)$는 B라는 사건이 일어났을 때 A 사건이 일어날 확률

$$P(A\cap B) = P(B)P(A|B)$$

- A라는 새로운 정보가 주어졌을 때 $P(B)$로부터 $P(B|A)$를 계산하는 방법을 제공한다.

$$P(B|A) = \dfrac {P(A\cap B)} {P(A)} = P(B) \dfrac {P(A|B)} {P(A)}$$

- 베이즈 정리는 조건부확률을 이용하여 **정보를 갱신하는 방법**을 알려준다.

- [ ]  모수란?

모수는 모집단의 특성(모평균, 모분산 등)을 나타내는 값이다. 이 값은 모집단을 전수조사해야만 알 수 있는 값이다. 그러나 실질적으로 모집단이 워낙 방대하므로 표본을 조사해 표본평균, 표본분산 등으로 모평균, 모분산 등을 추정할 수 있다.

![Untitled.png](/images/2021-02-02/25/Untitled.png)
- 사후확률 : **데이터를 측정한 이후**의 확률
- 사전확률 : **데이터가 주어지지 않은 상황**에서 모델링을 하기 이전에 사전에 주어진 확률

사전 확률로부터 시작해 데이터를 측정하면서 베이즈 정리를 통해 확률을 업데이트할 수 있다.

- 가능도 : 현재의 가정(모수 혹음 패러미터)에서 데이터가 관측될 확률
- evidence : 데이터 전체의 분포
- 베이즈 통계학에서는 사전확률이 상당히 중요한 정보이다. 만약 사전확률이 없을 경우, 임의값을 사용하기도 하지만 이는 분석의 신뢰도가 매우 떨어지는 결과를 낳을 수 있다.

## 베이즈 정리: 예제

![Untitled%201.png](/images/2021-02-02/25/Untitled%201.png)
- [ ]  $¬$은 not을 의미한다. 즉 $P(\mathscr D|¬\theta)$은 $\theta$가 일어나지 않았을 경우, $\mathscr D$가 일어났을 확률을 이야기 한다.
- 오탐률이 오르면 테스트의 정밀도가 떨어진다.

## 조건부 확률의 시각화

![Untitled%202.png](/images/2021-02-02/25/Untitled%202.png)
- 일반적으로 2종 오류의 경우 심각한 문제를 야기하는 경우가 많기 때문에 1종 오류에서 희생을 보더라도 2종 오류를 줄이는 쪽으로 설계를 많이 한다.

## 베이즈 정리를 통한 정보의 갱신

- 베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 계산할 수 있다.

![Untitled%203.png](/images/2021-02-02/25/Untitled%203.png)
- 새로운 데이터가 들어왔을 때는 앞서 계산한 사후확률을 사전확률로 사용하고, evidence를 계산해 사후확률을 계산한다.
- 베이즈 정리의 장점은 새롭게 들어온 데이터를 이용해 정보를 업데이트할 수 있는 것에 있다.

## 조건부확률 → 인과관계

- 조건부 확률은 유용한 통계적 해석을 제공하지만 인과관계(causality)를 추론할 때 함부로 사용해서는 안된다. 데이터가 많아져도 조건부확률만으로는 인과관계를 예측할수는 없다.
- 인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 필요하다.
- 조건부확률 기반의 예측모형은 새로운 유형의 데이터에 민감하게 정확도가 떨어질 수 있다.
- 인과관계만 고려해서는 높은 예측정확도를 보장할 수 없다. 하지만 다양한 시나리오에 정확도가 민감하게 변하지 않는다.
- 중첩 요인(confounding factor)의 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산해야 한다. 만일 중첩요인의 효과를 제거하지 않으면 가짜 연관성(spurious correlation)이 나온다.

## 인과관계 추론: 예제

![Untitled%204.png](/images/2021-02-02/25/Untitled%204.png)
# Deep Learning Basic

## key components of deep learning

- The **Data** that the model can learn form
- The **Model** how to transform the data
- The **Loss** function that quantifies the badness of the model
- The **Algorithm** to adjust the parameters to minimize the loss

# Pytorch

- Numpy 구조를 가지는 Tensor 객체로 array 표현
- 자동미분을 지원하여 DL 연산을 지원
- 다양한 형태의 DL을 지원하는 함수와 모델을 지원한다.

# Neural Networks

- 인간의 신경망에 영향을 받아 만든 컴퓨터 시스템이다.
- Neural networks are function approximators that stack affine transformations followed by nonlinear transformations.

# Further homework

noMNIST를 주어진 파이토치 코드를 이용해 학습시켜보기

데이터셋: [http://yaroslavvb.com/upload/notMNIST/](http://yaroslavvb.com/upload/notMNIST/)
