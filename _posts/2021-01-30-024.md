---
title: 10일차 - AI를 위한 수학5 및 시각화 도구
tags: [boostcamp]
published: true
use_math: true
comments: true
---

# 10일차 - 시각화 도구와 통계학

# matplotlib

- 다양한 graph 지원, pandas 지원
- pyplot 객체를 사용하여 데이터를 표시
- pyplot 객체에 그래프들을 쌓은 다음 flush

```python
import matplotlib.pyplot as plt

x = range(100)
y = [value**2 for value in x]
plt.plot(x, y)
plt.show() 
```

- argument를 kwargs로 받음
- 고정된 argument가 없어 레퍼런스를 보며 공부해야됨
- 이 때 중요한 것은 넘겨줄 데이터가 반드시 **오름차순**이어야 한다.
- show를 하거나 cla함수 또는 savefig함수를 사용하면 모든 객체가 flush된다.

## matplotlib의 기본적인 구조

![Untitled.png](/images/2021-01-30/Untitled.png)
```python
import numpy as np
x_1 = [x for x in range(0,100)]

y_1 = [x*2 for x in x_1]
y_2 = [x**2 for x in x_1]
y_3 = [np.sqrt(x) for x in x_1]
plt.plot(x_1, y_1)
plt.plot(x_1, y_2)
plt.plot(x_1, y_3)
```

![Untitled%201.png](/images/2021-01-30/Untitled%201.png)
```python
fig = plt.figure() # figure 반환
fig.set_size_inches(10, 5) # 크기 지정
ax_1 = fig.add_subplot(1, 2, 1) # plot 생성
ax_2 = fig.add_subplot(1, 2, 2) 
'''
이 때 앞의 두 값은 보여줄 grid의 형태(행과 열)를 의미 한다.
같이 보여 줄 plot들은 모두 앞의 두 수가 일치 해야한다.
그리고 마지막 번호가 grid의 인덱스를 의미한다.
모르겠다면 해볼것
'''

ax_1.plot(x_1, y_1, c='b') # c는 색을 의미
ax_2.plot(x_2, y_2, color='#00ff00') # 이러게 RGB값으로도 색 지정가능
plt.show()
```

![Untitled%202.png](/images/2021-01-30/Untitled%202.png)
## Style

```python
plt.plot(x_1, y_1, linestyle='dashed', label='1-dimesion')
plt.plot(x_1, y_2, ls='dashdot', label='2-dimension')
'''
linestyle, label 설정
'''

plt.title('$\mathbb {S}\cup C \mathscr {K}$') # 제목 수식도 가능
plt.legend(shadow=True, fancybox=True, loc='lower right') # 범례

plt.show()
```

![Untitled%203.png](/images/2021-01-30/Untitled%203.png)
```python
plt.plot(x_1, y_1)
plt.plot(x_1, y_2)

plt.text(x_1[50], y_1[50], 'Line_1') # 텍스트 위치
plt.annotate(
    'Line_2',
    xy=(x_1[50], y_2[50]), # 화살표로 가리키는 위치
    xytext=(30, 4000), # 텍스트 위치
    arrowprops=dict(facecolor='black', shrink=0.05)
)

plt.show()
```

![Untitled%204.png](/images/2021-01-30/Untitled%204.png)
```python
plt.style.use('ggplot') 
# 스타일 적용, 레퍼런스에 가면 많은 좋은 스타일이 있다.
plt.plot(x_1, y_1)
plt.show()
```

![Untitled%205.png](/images/2021-01-30/Untitled%205.png)
```python
plt.plot(x_1, y_1)
plt.grid(True, ls='--', c='0.7') # 그리드 표현 여부 c는 투명도

plt.xlim(-20, 1000) # x 표현 범위
plt.ylim(-20, 1000) # y 표현 범위

plt.show()
```

![Untitled%206.png](/images/2021-01-30/Untitled%206.png)
# scatter

- scatter 함수를 사용한다.
- marker: scatter 모양 지정

```python
N = 50
x = np.random.rand(N)
y = np.random.rand(N)
colors = np.random.rand(N)
area = np.pi * (15 * np.random.rand(N))**2
plt.scatter(x, y, s=area, c=colors, alpha=0.5) 
# alpha는 투명도
# s는 넓이 지정, s 역시 시퀀스형 데이터
plt.show()
```

![Untitled%207.png](/images/2021-01-30/Untitled%207.png)
# bar

```python
data = [[5,25,50,20],
        [4,23,51,17],
        [6,22,52,19]]

x = np.arange(4)
plt.bar(x + 0.00, data[0], color='b', width=0.25)
plt.bar(x + 0.25, data[1], color='g', width=0.25)
plt.bar(x + 0.50, data[2], color='r', width=0.25)
# 이 때 x값에 더해주는 값은 위치값이다.
# 바텀값을 주면 위치를 위,아래로 조절할 수 있다.
# barh함수로 수평막대를 그릴 수도 있다.
plt.xticks(X+0.25, ('A','B','C','D')) 
# x축에 이름 달아주기
plt.show()
```

![Untitled%208.png](/images/2021-01-30/Untitled%208.png)
# histogram

```python
x = np.random.normal(0, 100, 1000)
plt.hist(x, color='y', bins=20) # bins는 막대 개수
plt.show()
```

![Untitled%209.png](/images/2021-01-30/Untitled%209.png)
# boxplot

![Untitled%2010.png](/images/2021-01-30/Untitled%2010.png)
```python
data = np.random.randn(100, 5) # 100 * 5의 데이터
plt.boxplot(data)
plt.show()
```

![Untitled%2011.png](/images/2021-01-30/Untitled%2011.png)
# seaborn

- 기존 matplotlib의 wrapper라고 보면된다.
- 간단한 코드와 예쁜 결과를 보여준다.
- 강의는 seaborn tutorial을 참조

```python
import seaborn as sns

sns.set(style='darkgrid') # 스타일 정하기
tips = sns.load_dataset('tips')
fmri = sns.load_dataset('fmri')
sns.lineplot(x='timepoint', y='signal', data=fmri)
'''
기본적으로 x와 y에는 데이터가 들어가야 하지만
data변수에 데이터프레임을 넘기면 
x와 y에는 시각화할 column 값을 넘긴다.
'''
```

![Untitled%2012.png](/images/2021-01-30/Untitled%2012.png)
```python
sns.lineplot(x='timepoint', y='signal', hue='event', data=fmri)
# hue에는 원하는 column을 구분해 시각화 할 수 있다.
```

![Untitled%2013.png](/images/2021-01-30/Untitled%2013.png)
```python
sns.scatterplot(x='total_bill', y='tip', data=tips)
```

![Untitled%2014.png](/images/2021-01-30/Untitled%2014.png)
```python
sns.regplot(x='total_bill', y='tip', data=tips) # 선형 회귀
```

![Untitled%2015.png](/images/2021-01-30/Untitled%2015.png)
```python
sns.countplot(x='smoker', hue='time', data=tips) # bar chart
```

![Untitled%2016.png](/images/2021-01-30/Untitled%2016.png)
이 외에도 distplot, violinplot, stripplot, swarmplot 등 다양한 기능들이 있다.

# 통계학

## 모수

- **통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)**하는 것이 목표이며, 기계학습과 통계학이 공통적으로 추구하는 목표이다.
- 그러나 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알아낸다는 것은 불가능하므로, **근사적으로 확률분포를 추정**할 수 밖에 없다.
- 예측 모형의 목적은 분포를 정확하게 맞추는 것보다 **데이터와 추정 방법의 불확실성을 고려해서 위험을 최소화** 하기 위한 것
- 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 **모수적(parametric) 방법론**이라 한다.
- 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개사구 유연하게 바뀌면 **비모수(nonparametric) 방법론**이라 부른다.
- 비모수 방법론은 **모수를 사용하지 않는 것이 아니라는 것**을 명심할 것

## 확률을 가정하기

우선 히스토그램을 통해 모양을 관찰한다.

- 2개의 값만 가진다. → 베르누이분포
- 데이터가 n개의 이산적인 값을 가지는 경우 → 카테고리 분포, 다항 분포
- 데이터가 [0,1] 사이에서 값을 가지는 경우 → 베타분포
- 데이터가 0 이상의 값을 가지는 경우 → 감마분포, 로그정규분포 등
- 데이터가 $\mathbb R$ 전체에서 값을 가지는 경우 → 정규분포, 라플라스분포 등

기계적으로 확률 분포를 가정해서는 안 된다. 반드시 **데이터를 생성하는 원리를 먼저 고려하는 것이 원칙**이다. 모수를 추정한 후에는 반드시 검정을 해야 한다.

## 모수를 추정하자.

![Untitled%2017.png](/images/2021-01-30/Untitled%2017.png)
- **통계량의 확률분포를 표집분포(sampling distribution)**라 부르며, 특히 표본평균의 표집분표는 $N$이 커질수록 정규분포 $\mathscr N(\mu, \sigma^2|N)$을 따른다.
- 표본 분표(sample distribution)과 표본 분표(sampling distribution)은 다른 용어이다.
- 이를 **중심극한정리(central limit theorem)**이라 부르며, 모집단의 분포가 정규분포를 따르지 않아도 성립한다.

## 최대가능도 추정법

![Untitled%2018.png](/images/2021-01-30/Untitled%2018.png)
![Untitled%2019.png](/images/2021-01-30/Untitled%2019.png)
## 왜 로그가능도를 사용하는가?

- 로그가능도를 최적화하는 모수 $\theta$는 가능도를 최적화하는 MLE가 된다.
- 데이터의 숫자가 적으면 상관없지만 만일 데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것은 불가능하다.
- 데이터가 독립일 경우, 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 컴퓨터로 연산이 가능해진다.
- 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하게 되는데, 로그 가능도를 사용하면 연산량을 $O(N^2)$에서 $O(N)$으로 줄여준다.
- 대개의 손실함수의 경우 경사하강법을 사용하므로 음의 로그가능도(negative log-likelihood)를 최적화하게 된다.

## 최대가능도 추정법 예제: 정규분포

![Untitled%2020.png](/images/2021-01-30/Untitled%2020.png)
![Untitled%2021.png](/images/2021-01-30/Untitled%2021.png)
![Untitled%2022.png](/images/2021-01-30/Untitled%2022.png)
- 표본평균을 구하는 것과 최대가능도를 구하는 것은 같다.

## 최대가능도 추정법 예제: 카테고리 분포

![Untitled%2023.png](/images/2021-01-30/Untitled%2023.png)
![Untitled%2024.png](/images/2021-01-30/Untitled%2024.png)
![Untitled%2025.png](/images/2021-01-30/Untitled%2025.png)
![Untitled%2026.png](/images/2021-01-30/Untitled%2026.png)
- 카테고리 분포의 MLE는 경우의 수를 세어서 비율을 구하는 것이다.

## 딥러닝에서 최대가능도 추정법

![Untitled%2027.png](/images/2021-01-30/Untitled%2027.png)
## 확률분포의 거리

- 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도한다.
- 데이터 공간에 두 개의 확률분포 $P(x)$, $Q(x)$가 있을 경우 두 확률분포 사이의 거리를 계산할 때 다음과 같은 함수들을 이용한다.
1. 총변동 거리 (Total Variation Distance, TV)
2. 쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)
3. 바슈타인 거리 (Wasserstein Distance)

## 쿨백-라이블러 발산

![Untitled%2028.png](/images/2021-01-30/Untitled%2028.png)
- 분류 문제에서 정답레이블을 P, 모델 예측을 Q라 두면 **최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것**과 같다.
- 이는 거리 개념이 아니며 $\mathbb {KL}(P\|\|Q) \ne \mathbb {KL}(Q\|\|P)$ 임을 주의하라

![Untitled%2029.png](/images/2021-01-30/Untitled%2029.png)
