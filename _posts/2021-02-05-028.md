---
title: 14일차 - RNN과 Transformer
tags: [boostcamp]
published: true
use_math: true
comments: true
---

# RNN 첫걸음

## 시퀀스 데이터 이해하기

- 소리 문자열 주가 등의 데이터를 시퀀스(sequence) 데이터로 분류한다.
- 시퀀스 데이터는 독립동등분포(i.i.d.) 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다.
- 이런 시퀀스 데이터는 과거의 정보 혹은 앞,뒤 맥락이 굉장히 중요하기 때문에 위치를 바꾸는 것과 같은 인위적인 조작에 큰 주의를 요한다.

## 시퀀스 데이터 다루기

- 이전 시퀀스 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있다.
- 베이즈 법칙을 이용해 다음과 같이 식을 전개한다.

![Untitled.png](/images/2021-02-05/028/Untitled.png)
- 하지만 과거의 모든 정보를 사용할 필요는 없다.
- 시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요하다.

![Untitled%201.png](/images/2021-02-05/028/Untitled%201.png)
- $\tau$의 경우에도 Hyper-parameter에 해당되므로 이를 사전에 정해주어야 한다. 이를 정해주는데에도 사전지식이 필요할 때도 있다.
- 또한 $\tau$가 바뀌어야할 때도 있다.

![Untitled%202.png](/images/2021-02-05/028/Untitled%202.png)
- 이러한 방법은 가변적인 길이의 데이터를 고정적인 형태로 바꿀 수 있다는 장점이 있다. 또한 과거의 모든 데이터를 결과에 반영할 수 있다.

![Untitled%203.png](/images/2021-02-05/028/Untitled%203.png)
## Recurrent Neural Network

![Untitled%204.png](/images/2021-02-05/028/Untitled%204.png)
- RNN은 이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링한다.
- 이때, 주의 해야할 것은 각 가중치들은 다음 시퀀스에서도 불변하다는 사실이다. 각 시퀀스마다 변하는 것은 입력, 잠재변수 H 뿐이다.

## Backpropagation Through Time(BPTT)

- RNN의 backpropagation으로 시퀀스적으로 연산되는 네트워크이기 때문에 역전파과정도 다른 네트워크와는 다르다.

![Untitled%205.png](/images/2021-02-05/028/Untitled%205.png)
- 해당 항은 잠재변수의 미분값을 계속 곱해줘야 하기 때문에 1보다 커지면 계속 커지고 1보다 작아면 계속 작아지기 때문에, 매우 불안정하다.
- 그래디언트 소실을 막기 위해 일부 그래디언트 값을 끊고 과거의 정보를 몇몇 블록으로 나눠서 역전파를 하는 방법인 Truncated BPTT가 있다.
- 이를 해결하기 위해 등장한 RNN 네트워크가 LSTM과 GRU이다.
- 자세한 것은 나중에 공부해 볼 것

# Sequential Model

- Naive sequence model
- 몇 개의 입력이 들어올 지 모르니 기존의 모델로는 데이터를 처리할 수 없다.

## Autogressive model

![Untitled%206.png](/images/2021-02-05/028/Untitled%206.png)
- 현재의 결과에 반영할 과거의 데이터의 크기를 고정한다.

## Markov model (first-order autoregressive model)

![Untitled%207.png](/images/2021-02-05/028/Untitled%207.png)
- 바로 전의 데이터만 현재의 결과에 반영한다.
- 이는 식으로 표현하기 매우 쉽지만 현실을 반영하지 못 한다.

## Latent autoregressive model

![Untitled%208.png](/images/2021-02-05/028/Untitled%208.png)
- 과거에 대한 요약을 잠재 변수 H에 저장해 현재의 결과에 반영한다.

# RNN

![Untitled%209.png](/images/2021-02-05/028/Untitled%209.png)
- RNN은 과거로부터 멀어지면 그 정보에 대한 반영이 점점 줄어들게 된다.

# Long Short Term Memory

![Untitled%2010.png](/images/2021-02-05/028/Untitled%2010.png)
![Untitled%2011.png](/images/2021-02-05/028/Untitled%2011.png)
![Untitled%2012.png](/images/2021-02-05/028/Untitled%2012.png)
- 셀 스테이트를 통해 먼 과거의 정보도 현실 결과에 반영할 수 있게 되었다.
- LSTM의 각 게이트들은 사실 알고보면 덴스넷이며 parameter가 많다.

## Gated Recurrent Unit

![Untitled%2013.png](/images/2021-02-05/028/Untitled%2013.png)
- LSTM의 게이트를 줄여보고자 나온 모델이다.

# Transformer

![Untitled%2014.png](/images/2021-02-05/028/Untitled%2014.png)
- 여기서 Transform한다는 것은 Sequence to Sequence 즉 다른 Sequence 데이타로 변환한다는 뜻이다.

![Untitled%2015.png](/images/2021-02-05/028/Untitled%2015.png)
- Feed Foward Neural Network 는 일반적인 MLP와 구조가 비슷하고 Transformer에서 중요한 것은 Self-Attension이다.

![Untitled%2016.png](/images/2021-02-05/028/Untitled%2016.png)
- Self-attention을 거치면 각 단어의 수만큼 벡터가 출력되고 이 값은 서로 의존적이다.

![Untitled%2017.png](/images/2021-02-05/028/Untitled%2017.png)
- Self-attention에서는 예시와 같이 it을 추론할 수 있도록 학습된다.

![Untitled%2018.png](/images/2021-02-05/028/Untitled%2018.png)
- 위와 같이 'Thinking' 와 'Machines'라는 두 단어가 있다고 생각했을 때, 각 Queries, Keys, Values 세 개의 벡터를 계산한다.
- 그 다음 한 단어의 q벡터에 대해 모든 단어의 key값을 내적해 Score를 계산한다. 그리고 그 값을 $\sqrt{d_k}$로 나누어 일정한 범위 안에 들도록 한다. 이 때 ${d_k}$은 Keys 벡터의 dim값이다.
- 그리고 그 값을 소프트맥스로 연산해 각 단어들과의 관련도를 예측한다.

![Untitled%2019.png](/images/2021-02-05/028/Untitled%2019.png)
- 소프트맥스 값을 각 Vaule 벡터에 곱하고 더해주면 출력값이 나온다.
- 이 때 서로 내적해야 되는 q와 k의 dim은 항상 같아야 한다.
- 위의 연산은 다음과 같은 수식으로 정리된다.

![Untitled%2020.png](/images/2021-02-05/028/Untitled%2020.png)
- Transformer는 다른 단어들의 영향을 받으므로 기존의 모델에 비해 flexible하지만 그만큼 수많은 벡터를 만들어야 하기 때문에 메모리를 많이 요구하게 된다.

## Multi-head Attention

- 한 벡터에 대해서 여러 개의 Attention을 거치는 것

![Untitled%2021.png](/images/2021-02-05/028/Untitled%2021.png)
- 하지만 이렇게 하면 출력과 입력의 dims가 차이가 난다.

![Untitled%2022.png](/images/2021-02-05/028/Untitled%2022.png)
- 그래서 모든 값을 concat한 후 W벡터를 곱해 적절한 dim로 출력을 맞춰준다.

## Position encoding

![Untitled%2023.png](/images/2021-02-05/028/Untitled%2023.png)
- Transformer는 각 단어의 순서에 대해 독립적으로 결과가 나온다. 단어의 순서가 바뀌어도 계산 값은 다르지 않다. 그래서 위치에 대한 정보를 더해준다.

## Decoder

![Untitled%2024.png](/images/2021-02-05/028/Untitled%2024.png)
- 인코더에서는 K벡터와 V벡터를 디코더에 전달한다.

![Untitled%2025.png](/images/2021-02-05/028/Untitled%2025.png)
- 인코더에서는 각 입력값에 대해 미래의 단어들에 대해 영향을 받지 않도록 소프트맥스 전 단계에서 미래의 값을 마스킹한다.
- Encoder-Decoder Attention과 Self-Attention의 차이점은 K와 V벡터를 인코더로부터 받는다는 것과, 쿼리벡터를 만들지 않는다는 것이다.

## OutPut Layer

![Untitled%2026.png](/images/2021-02-05/028/Untitled%2026.png)
- 자세한 것은 나중에 찾아볼 것
